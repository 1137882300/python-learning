{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-19T01:27:05.760028Z",
     "start_time": "2024-07-19T01:27:05.008374Z"
    }
   },
   "outputs": [],
   "source": [
    "# 基本配置\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "qw_llm_openai = ChatOpenAI(\n",
    "    openai_api_base=os.getenv('DASHSCOPE_API_BASE'),\n",
    "    openai_api_key=os.getenv('DASHSCOPE_API_KEY'),\n",
    "    model_name=\"qwen2-1.5b-instruct\",\n",
    "    temperature=0,\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "ms_llm_openai = ChatOpenAI(\n",
    "    openai_api_base=os.getenv('MOONSHOT_API_BASE'),\n",
    "    openai_api_key=os.getenv('MOONSHOT_API_KEY'),\n",
    "    model_name=\"moonshot-v1-8k\",\n",
    "    temperature=0,\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "cf_llm_openai = ChatOpenAI(\n",
    "    openai_api_base=os.getenv('CF_API_BASE'),\n",
    "    openai_api_key=os.getenv('CF_API_TOKEN'),\n",
    "    model_name=\"@cf/meta/llama-3-8b-instruct\",\n",
    "    temperature=0,\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "groq_llm_openai = ChatOpenAI(\n",
    "    openai_api_base=os.getenv('GROQ_API_BASE'),\n",
    "    openai_api_key=os.getenv('GROQ_API_KEY'),\n",
    "    model_name=\"llama3-8b-8192\",\n",
    "    temperature=0,\n",
    "    streaming=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "如果我们想要的模型返回Pydantic目的，我们只是需要通过在所需Pydantic类："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ce1198d2153b33a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Joke(setup=\"Why don't cats make good dancers?\", punchline='Because they already have four left feet.', rating=None)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    rating: Optional[int] = Field(description=\"How funny the joke is, from 1 to 10\")\n",
    "\n",
    "\n",
    "structured_llm = qw_llm_openai.with_structured_output(Joke)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T01:27:06.711952Z",
     "start_time": "2024-07-19T01:27:05.761510Z"
    }
   },
   "id": "ead779fe47f40da1",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们还可以通过在一个\"JSON Schema\"结束的字典如果你不喜欢使用Pydantic. 在这种情况下，回应也是一个字典:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "868b4f4b8abe9f0d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'setup': 'Why did the cat join a band?',\n 'punchline': 'Because it wanted to be the purr-cussionist!',\n 'rating': 8}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_schema = {\n",
    "    \"title\": \"joke\",\n",
    "    \"description\": \"Joke to tell user.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"setup\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The setup of the joke\",\n",
    "        },\n",
    "        \"punchline\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The punchline to the joke\",\n",
    "        },\n",
    "        \"rating\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"How funny the joke is, from 1 to 10\",\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"setup\", \"punchline\"],\n",
    "}\n",
    "structured_llm = groq_llm_openai.with_structured_output(json_schema)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T01:28:01.683171Z",
     "start_time": "2024-07-19T01:27:57.827628Z"
    }
   },
   "id": "11a95f22f64990d7",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Choosing between multiple schemas\n",
    "选择之间的多种模式\n",
    "\n",
    "最简单的方式，让该模型的选择从多个模式是创造一个父母Pydantic类有一个联盟类型的属性："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41d38b9b016c9196"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Response(output=Joke(setup=\"Why don't cats make good dancers?\", punchline='Because they always fall flat on their faces!', rating=None))"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "class ConversationalResponse(BaseModel):\n",
    "    \"\"\"Respond in a conversational manner. Be kind and helpful.\"\"\"\n",
    "\n",
    "    response: str = Field(description=\"A conversational response to the user's query\")\n",
    "\n",
    "\n",
    "class Response(BaseModel):\n",
    "    output: Union[Joke, ConversationalResponse]\n",
    "\n",
    "\n",
    "structured_llm = qw_llm_openai.with_structured_output(Response)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T01:48:26.889951Z",
     "start_time": "2024-07-19T01:48:26.094185Z"
    }
   },
   "id": "6537481e35945f9f",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for Response\noutput\n  value is not a valid dict (type=type_error.dict)\noutput\n  value is not a valid dict (type=type_error.dict)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValidationError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mstructured_llm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mHow are you today?\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:2824\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m   2822\u001B[0m             \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m step\u001B[38;5;241m.\u001B[39minvoke(\u001B[38;5;28minput\u001B[39m, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   2823\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2824\u001B[0m             \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mstep\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2825\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[1;32m   2826\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/langchain_core/output_parsers/base.py:183\u001B[0m, in \u001B[0;36mBaseOutputParser.invoke\u001B[0;34m(self, input, config)\u001B[0m\n\u001B[1;32m    179\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Union[\u001B[38;5;28mstr\u001B[39m, BaseMessage], config: Optional[RunnableConfig] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    181\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m    182\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, BaseMessage):\n\u001B[0;32m--> 183\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_with_config\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    184\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43minner_input\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse_result\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[43m                \u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatGeneration\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minner_input\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    186\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    187\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    188\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    189\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mparser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    190\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    191\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    192\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_with_config(\n\u001B[1;32m    193\u001B[0m             \u001B[38;5;28;01mlambda\u001B[39;00m inner_input: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparse_result([Generation(text\u001B[38;5;241m=\u001B[39minner_input)]),\n\u001B[1;32m    194\u001B[0m             \u001B[38;5;28minput\u001B[39m,\n\u001B[1;32m    195\u001B[0m             config,\n\u001B[1;32m    196\u001B[0m             run_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparser\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    197\u001B[0m         )\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:1734\u001B[0m, in \u001B[0;36mRunnable._call_with_config\u001B[0;34m(self, func, input, config, run_type, **kwargs)\u001B[0m\n\u001B[1;32m   1730\u001B[0m     context \u001B[38;5;241m=\u001B[39m copy_context()\n\u001B[1;32m   1731\u001B[0m     context\u001B[38;5;241m.\u001B[39mrun(_set_config_context, child_config)\n\u001B[1;32m   1732\u001B[0m     output \u001B[38;5;241m=\u001B[39m cast(\n\u001B[1;32m   1733\u001B[0m         Output,\n\u001B[0;32m-> 1734\u001B[0m         \u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1735\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcall_func_with_variable_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   1736\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   1737\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[1;32m   1738\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1739\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1740\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1741\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m   1742\u001B[0m     )\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1744\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/langchain_core/runnables/config.py:379\u001B[0m, in \u001B[0;36mcall_func_with_variable_args\u001B[0;34m(func, input, config, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    377\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m accepts_run_manager(func):\n\u001B[1;32m    378\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m run_manager\n\u001B[0;32m--> 379\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/langchain_core/output_parsers/base.py:184\u001B[0m, in \u001B[0;36mBaseOutputParser.invoke.<locals>.<lambda>\u001B[0;34m(inner_input)\u001B[0m\n\u001B[1;32m    179\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[1;32m    180\u001B[0m     \u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Union[\u001B[38;5;28mstr\u001B[39m, BaseMessage], config: Optional[RunnableConfig] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    181\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m    182\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, BaseMessage):\n\u001B[1;32m    183\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_with_config(\n\u001B[0;32m--> 184\u001B[0m             \u001B[38;5;28;01mlambda\u001B[39;00m inner_input: \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse_result\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[43m                \u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatGeneration\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minner_input\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    186\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    187\u001B[0m             \u001B[38;5;28minput\u001B[39m,\n\u001B[1;32m    188\u001B[0m             config,\n\u001B[1;32m    189\u001B[0m             run_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparser\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    190\u001B[0m         )\n\u001B[1;32m    191\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    192\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_with_config(\n\u001B[1;32m    193\u001B[0m             \u001B[38;5;28;01mlambda\u001B[39;00m inner_input: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparse_result([Generation(text\u001B[38;5;241m=\u001B[39minner_input)]),\n\u001B[1;32m    194\u001B[0m             \u001B[38;5;28minput\u001B[39m,\n\u001B[1;32m    195\u001B[0m             config,\n\u001B[1;32m    196\u001B[0m             run_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparser\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    197\u001B[0m         )\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/langchain_core/output_parsers/openai_tools.py:299\u001B[0m, in \u001B[0;36mPydanticToolsParser.parse_result\u001B[0;34m(self, result, partial)\u001B[0m\n\u001B[1;32m    297\u001B[0m             \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m    298\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 299\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    300\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfirst_tool_only:\n\u001B[1;32m    301\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pydantic_objects[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m pydantic_objects \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/langchain_core/output_parsers/openai_tools.py:294\u001B[0m, in \u001B[0;36mPydanticToolsParser.parse_result\u001B[0;34m(self, result, partial)\u001B[0m\n\u001B[1;32m    289\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(res[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124margs\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m    290\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    291\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTool arguments must be specified as a dict, received: \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    292\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mres[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124margs\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    293\u001B[0m         )\n\u001B[0;32m--> 294\u001B[0m     pydantic_objects\u001B[38;5;241m.\u001B[39mappend(\u001B[43mname_dict\u001B[49m\u001B[43m[\u001B[49m\u001B[43mres\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtype\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mres\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43margs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    295\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ValidationError, \u001B[38;5;167;01mValueError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    296\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m partial:\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/pydantic/main.py:341\u001B[0m, in \u001B[0;36mpydantic.main.BaseModel.__init__\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mValidationError\u001B[0m: 2 validation errors for Response\noutput\n  value is not a valid dict (type=type_error.dict)\noutput\n  value is not a valid dict (type=type_error.dict)"
     ]
    }
   ],
   "source": [
    "structured_llm.invoke(\"How are you today?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T01:50:02.928286Z",
     "start_time": "2024-07-19T01:50:02.111462Z"
    }
   },
   "id": "5bf303aa400060a7",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "Streaming 流\n",
    "We can stream outputs from our structured model when the output type is a dict (i.e., when the schema is specified as a JSON Schema dict).\n",
    "我们可以流产出我们的结构模型的输出类型是字典(即，当架构被指定为\"}\"结束词典)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8a6b7b638a6713e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'setup': \"Why don't\"}\n",
      "{'setup': \"Why don't cats make good dancers?\"}\n",
      "{'setup': \"Why don't cats make good dancers?\", 'punchline': 'Because they already have four'}\n",
      "{'setup': \"Why don't cats make good dancers?\", 'punchline': 'Because they already have four left feet!'}\n"
     ]
    }
   ],
   "source": [
    "structured_llm = qw_llm_openai.with_structured_output(json_schema)\n",
    "\n",
    "for chunk in structured_llm.stream(\"Tell me a joke about cats\"):\n",
    "    print(chunk)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T01:51:47.739567Z",
     "start_time": "2024-07-19T01:51:47.054336Z"
    }
   },
   "id": "4a42162bcfe127b6",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "Few-shot prompting 几拍提示\n",
    "最简单和最普遍的方法是添加实例，以一个系统信息的提示："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6d2c1e2ab3ee065"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'setup': 'Woodpecker', 'punchline': \"They're known for their peeling nuts!\"}"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\\n",
    "Return a joke which has the setup (the response to \"Who's there?\") and the final punchline (the response to \"<setup> who?\").\n",
    "\n",
    "Here are some examples of jokes:\n",
    "\n",
    "example_user: Tell me a joke about planes\n",
    "example_assistant: {{\"setup\": \"Why don't planes ever get tired?\", \"punchline\": \"Because they have rest wings!\", \"rating\": 2}}\n",
    "\n",
    "example_user: Tell me another joke about planes\n",
    "example_assistant: {{\"setup\": \"Cargo\", \"punchline\": \"Cargo 'vroom vroom', but planes go 'zoom zoom'!\", \"rating\": 10}}\n",
    "\n",
    "example_user: Now about caterpillars\n",
    "example_assistant: {{\"setup\": \"Caterpillar\", \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\", \"rating\": 5}}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", \"{input}\")])\n",
    "\n",
    "few_shot_structured_llm = prompt | structured_llm\n",
    "few_shot_structured_llm.invoke(\"what's something funny about woodpeckers\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T01:52:49.568316Z",
     "start_time": "2024-07-19T01:52:48.847552Z"
    }
   },
   "id": "606f59fe87e1cded",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "When the underlying method for structuring outputs is tool calling, we can pass in our examples as explicit tool calls. You can check if the model you're using makes use of tool calling in its API reference.\n",
    "当基础的方法的结构化产出的工具，呼吁，我们可以通过在我们的例子作为明确的工具的电话。 你可以检查，如果该模型使用的使用的工具，呼吁在其API reference。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6ef982a19227bde"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'code': 'invalid_parameter_error', 'param': None, 'message': 'field required: payload.input.messages.2.content & field required: payload.input.messages.5.content & field required: payload.input.messages.8.content', 'type': 'invalid_request_error'}, 'id': 'chatcmpl-5ae7322b-f6b8-98a6-9e66-cce34de896f6'}",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mBadRequestError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 67\u001B[0m\n\u001B[1;32m     63\u001B[0m prompt \u001B[38;5;241m=\u001B[39m ChatPromptTemplate\u001B[38;5;241m.\u001B[39mfrom_messages(\n\u001B[1;32m     64\u001B[0m     [(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msystem\u001B[39m\u001B[38;5;124m\"\u001B[39m, system), (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mplaceholder\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{examples}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m), (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{input}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)]\n\u001B[1;32m     65\u001B[0m )\n\u001B[1;32m     66\u001B[0m few_shot_structured_llm \u001B[38;5;241m=\u001B[39m prompt \u001B[38;5;241m|\u001B[39m structured_llm\n\u001B[0;32m---> 67\u001B[0m \u001B[43mfew_shot_structured_llm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcrocodiles\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mexamples\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mexamples\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:2824\u001B[0m, in \u001B[0;36mRunnableSequence.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m   2822\u001B[0m             \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m step\u001B[38;5;241m.\u001B[39minvoke(\u001B[38;5;28minput\u001B[39m, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   2823\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2824\u001B[0m             \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mstep\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2825\u001B[0m \u001B[38;5;66;03m# finish the root run\u001B[39;00m\n\u001B[1;32m   2826\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:5006\u001B[0m, in \u001B[0;36mRunnableBindingBase.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m   5000\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[1;32m   5001\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   5002\u001B[0m     \u001B[38;5;28minput\u001B[39m: Input,\n\u001B[1;32m   5003\u001B[0m     config: Optional[RunnableConfig] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   5004\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Optional[Any],\n\u001B[1;32m   5005\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Output:\n\u001B[0;32m-> 5006\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbound\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   5007\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5008\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_merge_configs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5009\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5010\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:265\u001B[0m, in \u001B[0;36mBaseChatModel.invoke\u001B[0;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[1;32m    254\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[1;32m    255\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    256\u001B[0m     \u001B[38;5;28minput\u001B[39m: LanguageModelInput,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    260\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    261\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseMessage:\n\u001B[1;32m    262\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[1;32m    264\u001B[0m         ChatGeneration,\n\u001B[0;32m--> 265\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    266\u001B[0m \u001B[43m            \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    267\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcallbacks\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtags\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmetadata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrun_name\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrun_id\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m    275\u001B[0m     )\u001B[38;5;241m.\u001B[39mmessage\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:698\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    690\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[1;32m    691\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    692\u001B[0m     prompts: List[PromptValue],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    695\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    696\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    697\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[0;32m--> 698\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_messages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:555\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    553\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n\u001B[1;32m    554\u001B[0m             run_managers[i]\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[0;32m--> 555\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    556\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    557\u001B[0m     LLMResult(generations\u001B[38;5;241m=\u001B[39m[res\u001B[38;5;241m.\u001B[39mgenerations], llm_output\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39mllm_output)  \u001B[38;5;66;03m# type: ignore[list-item]\u001B[39;00m\n\u001B[1;32m    558\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[1;32m    559\u001B[0m ]\n\u001B[1;32m    560\u001B[0m llm_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_combine_llm_outputs([res\u001B[38;5;241m.\u001B[39mllm_output \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results])\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:545\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    542\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(messages):\n\u001B[1;32m    543\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    544\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m--> 545\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_with_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    546\u001B[0m \u001B[43m                \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    547\u001B[0m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    548\u001B[0m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    549\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    550\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    551\u001B[0m         )\n\u001B[1;32m    552\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    553\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:770\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    768\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    769\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 770\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    771\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    772\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    773\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    774\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:545\u001B[0m, in \u001B[0;36mBaseChatOpenAI._generate\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    541\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstreaming:\n\u001B[1;32m    542\u001B[0m     stream_iter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_stream(\n\u001B[1;32m    543\u001B[0m         messages, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    544\u001B[0m     )\n\u001B[0;32m--> 545\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgenerate_from_stream\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstream_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    546\u001B[0m payload \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_request_payload(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    547\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpayload)\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:97\u001B[0m, in \u001B[0;36mgenerate_from_stream\u001B[0;34m(stream)\u001B[0m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_from_stream\u001B[39m(stream: Iterator[ChatGenerationChunk]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatResult:\n\u001B[1;32m     88\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Generate from a stream.\u001B[39;00m\n\u001B[1;32m     89\u001B[0m \n\u001B[1;32m     90\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;124;03m        ChatResult: Chat result.\u001B[39;00m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 97\u001B[0m     generation \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     98\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m generation:\n\u001B[1;32m     99\u001B[0m         generation \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(stream)\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/langchain_openai/chat_models/base.py:487\u001B[0m, in \u001B[0;36mBaseChatOpenAI._stream\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    485\u001B[0m payload \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_request_payload(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    486\u001B[0m default_chunk_class: Type[BaseMessageChunk] \u001B[38;5;241m=\u001B[39m AIMessageChunk\n\u001B[0;32m--> 487\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpayload\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m response:\n\u001B[1;32m    488\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m response:\n\u001B[1;32m    489\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(chunk, \u001B[38;5;28mdict\u001B[39m):\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/openai/_utils/_utils.py:277\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    275\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    276\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[0;32m--> 277\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/openai/resources/chat/completions.py:606\u001B[0m, in \u001B[0;36mCompletions.create\u001B[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[1;32m    573\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    574\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate\u001B[39m(\n\u001B[1;32m    575\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    604\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m NOT_GIVEN,\n\u001B[1;32m    605\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatCompletion \u001B[38;5;241m|\u001B[39m Stream[ChatCompletionChunk]:\n\u001B[0;32m--> 606\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    607\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/chat/completions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    608\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    609\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m    610\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmessages\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    611\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    612\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfrequency_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    613\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunction_call\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    614\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunctions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunctions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    615\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogit_bias\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    616\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    617\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    618\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    619\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mparallel_tool_calls\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mparallel_tool_calls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    620\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpresence_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    621\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mresponse_format\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    622\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseed\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    623\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstop\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    624\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    625\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    626\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtemperature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    627\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtool_choice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    628\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtools\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_logprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_logprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    630\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_p\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    631\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    632\u001B[0m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    633\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    634\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    635\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    636\u001B[0m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[1;32m    637\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    638\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mChatCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    639\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    640\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatCompletionChunk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    641\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/openai/_base_client.py:1240\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1226\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[1;32m   1227\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1228\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1235\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1236\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m   1237\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[1;32m   1238\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[1;32m   1239\u001B[0m     )\n\u001B[0;32m-> 1240\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/openai/_base_client.py:921\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m    912\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[1;32m    913\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    914\u001B[0m     cast_to: Type[ResponseT],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    919\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    920\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m--> 921\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    922\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    923\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    924\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    925\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    926\u001B[0m \u001B[43m        \u001B[49m\u001B[43mremaining_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mremaining_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    927\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/openai/_base_client.py:1020\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1017\u001B[0m         err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m   1019\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1020\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1022\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_response(\n\u001B[1;32m   1023\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[1;32m   1024\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1027\u001B[0m     stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[1;32m   1028\u001B[0m )\n",
      "\u001B[0;31mBadRequestError\u001B[0m: Error code: 400 - {'error': {'code': 'invalid_parameter_error', 'param': None, 'message': 'field required: payload.input.messages.2.content & field required: payload.input.messages.5.content & field required: payload.input.messages.8.content', 'type': 'invalid_request_error'}, 'id': 'chatcmpl-5ae7322b-f6b8-98a6-9e66-cce34de896f6'}"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n",
    "\n",
    "examples = [\n",
    "    HumanMessage(\"Tell me a joke about planes\", name=\"example_user\"),\n",
    "    AIMessage(\n",
    "        \"\",\n",
    "        name=\"example_assistant\",\n",
    "        tool_calls=[\n",
    "            {\n",
    "                \"name\": \"joke\",\n",
    "                \"args\": {\n",
    "                    \"setup\": \"Why don't planes ever get tired?\",\n",
    "                    \"punchline\": \"Because they have rest wings!\",\n",
    "                    \"rating\": 2,\n",
    "                },\n",
    "                \"id\": \"1\",\n",
    "            }\n",
    "        ],\n",
    "    ),\n",
    "    # Most tool-calling models expect a ToolMessage(s) to follow an AIMessage with tool calls.\n",
    "    ToolMessage(\"\", tool_call_id=\"1\"),\n",
    "    # Some models also expect an AIMessage to follow any ToolMessages,\n",
    "    # so you may need to add an AIMessage here.\n",
    "    HumanMessage(\"Tell me another joke about planes\", name=\"example_user\"),\n",
    "    AIMessage(\n",
    "        \"\",\n",
    "        name=\"example_assistant\",\n",
    "        tool_calls=[\n",
    "            {\n",
    "                \"name\": \"joke\",\n",
    "                \"args\": {\n",
    "                    \"setup\": \"Cargo\",\n",
    "                    \"punchline\": \"Cargo 'vroom vroom', but planes go 'zoom zoom'!\",\n",
    "                    \"rating\": 10,\n",
    "                },\n",
    "                \"id\": \"2\",\n",
    "            }\n",
    "        ],\n",
    "    ),\n",
    "    ToolMessage(\"\", tool_call_id=\"2\"),\n",
    "    HumanMessage(\"Now about caterpillars\", name=\"example_user\"),\n",
    "    AIMessage(\n",
    "        \"\",\n",
    "        tool_calls=[\n",
    "            {\n",
    "                \"name\": \"joke\",\n",
    "                \"args\": {\n",
    "                    \"setup\": \"Caterpillar\",\n",
    "                    \"punchline\": \"Caterpillar really slow, but watch me turn into a butterfly and steal the show!\",\n",
    "                    \"rating\": 5,\n",
    "                },\n",
    "                \"id\": \"3\",\n",
    "            }\n",
    "        ],\n",
    "    ),\n",
    "    ToolMessage(\"\", tool_call_id=\"3\"),\n",
    "]\n",
    "system = \"\"\"You are a hilarious comedian. Your specialty is knock-knock jokes. \\\n",
    "Return a joke which has the setup (the response to \"Who's there?\") \\\n",
    "and the final punchline (the response to \"<setup> who?\").\"\"\"\n",
    "\n",
    "# placeholder : 一个可用于传入消息列表的占位符。\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system), (\"placeholder\", \"{examples}\"), (\"human\", \"{input}\")]\n",
    ")\n",
    "few_shot_structured_llm = prompt | structured_llm\n",
    "few_shot_structured_llm.invoke({\"input\": \"crocodiles\", \"examples\": examples})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T01:59:43.516721Z",
     "start_time": "2024-07-19T01:59:42.048172Z"
    }
   },
   "id": "98e0a71f7d86e8a6",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "(Advanced) Specifying the method for structuring outputs\n",
    "(高级)指定方法的结构化产出\n",
    "\n",
    "对于模型的支持，多个装置的构造的产出(即，他们支持这两个工具叫。模式)，可以指定方法的使用 method= 参数。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "634f10b38e97a4e3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Joke(setup=\"Why don't cats make good dancers?\", punchline='Because they always land on their feet.', rating=None)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果使用JSON模式你必须仍然指定所需的方案模型的提示。 \n",
    "# 该架构你过来 with_structured_output 将只能用于分析的模型输出，它将不会被传递给模型的方式是与工具的呼吁。\n",
    "structured_llm = qw_llm_openai.with_structured_output(Joke, method=\"json_mode\")\n",
    "\n",
    "structured_llm.invoke(\n",
    "    \"Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T02:02:20.750456Z",
     "start_time": "2024-07-19T02:02:20.086754Z"
    }
   },
   "id": "85a951efa79f83b3",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "(Advanced) Raw outputs (高级)原产出\n",
    "\n",
    "你可以避免的提高和例外处理的原输出自己通过 include_raw=True . 这种变化的输出格式，以包含原始信息输出， parsed 值(如果成功的话)，以及由此产生的任何错误："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37f9bf8c9058c888"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'call_961db6d06251427596eaab', 'function': {'arguments': '{\"setup\": \"Why don\\'t cats make good dancers?\", \"punchline\": \"Because they always fall over!\"}', 'name': 'Joke'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'stop', 'model_name': 'qwen2-1.5b-instruct'}, id='run-167889f7-febe-472d-a8bf-3b69dea553b1-0', tool_calls=[{'name': 'Joke', 'args': {'setup': \"Why don't cats make good dancers?\", 'punchline': 'Because they always fall over!'}, 'id': 'call_961db6d06251427596eaab', 'type': 'tool_call'}]),\n 'parsed': Joke(setup=\"Why don't cats make good dancers?\", punchline='Because they always fall over!', rating=None),\n 'parsing_error': None}"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_llm = qw_llm_openai.with_structured_output(Joke, include_raw=True)\n",
    "\n",
    "structured_llm.invoke(\n",
    "    \"Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T02:04:57.325995Z",
     "start_time": "2024-07-19T02:04:56.543694Z"
    }
   },
   "id": "877d7f6de690a995",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prompting and parsing model outputs directly\n",
    "提示和分析模型产出的直接\n",
    "\n",
    "不是所有的模型支持 .with_structured_output() ，由于并非所有模型有的工具，叫函模式的支持。 \n",
    "对于这样的模式只需要直接提示模型使用一种特定的格式，并使用一个输出分析器来提取的结构化反应的原始模型的输出。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3d151dd3c597287"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using PydanticOutputParser 使用 PydanticOutputParser ​"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "173ba97ecf7c2c59"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    name: str = Field(..., description=\"The name of the person\")\n",
    "    height_in_meters: float = Field(\n",
    "        ..., description=\"The height of the person expressed in meters.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class People(BaseModel):\n",
    "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
    "\n",
    "    people: List[Person]\n",
    "\n",
    "\n",
    "# Set up a parser\n",
    "parser = PydanticOutputParser(pydantic_object=People)\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\",\n",
    "        ),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ").partial(format_instructions=parser.get_format_instructions())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T02:06:46.424524Z",
     "start_time": "2024-07-19T02:06:46.418802Z"
    }
   },
   "id": "e03caead59f9621c",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: Answer the user query. Wrap the output in `json` tags\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"description\": \"Identifying information about all people in a text.\", \"properties\": {\"people\": {\"title\": \"People\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/Person\"}}}, \"required\": [\"people\"], \"definitions\": {\"Person\": {\"title\": \"Person\", \"description\": \"Information about a person.\", \"type\": \"object\", \"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"The name of the person\", \"type\": \"string\"}, \"height_in_meters\": {\"title\": \"Height In Meters\", \"description\": \"The height of the person expressed in meters.\", \"type\": \"number\"}}, \"required\": [\"name\", \"height_in_meters\"]}}}\n",
      "```\n",
      "Human: Anna is 23 years old and she is 6 feet tall\n"
     ]
    }
   ],
   "source": [
    "query = \"Anna is 23 years old and she is 6 feet tall\"\n",
    "\n",
    "print(prompt.invoke(query).to_string())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T02:06:52.517646Z",
     "start_time": "2024-07-19T02:06:52.509295Z"
    }
   },
   "id": "4854d02d2954ef20",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "People(people=[Person(name='Anna', height_in_meters=1.875)])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | qw_llm_openai | parser\n",
    "\n",
    "chain.invoke({\"query\": query})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T02:07:28.333710Z",
     "start_time": "2024-07-19T02:07:27.424158Z"
    }
   },
   "id": "70f9a4ed9a9f6397",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "Custom Parsing 定义的分析\n",
    "\n",
    "你还可以创建一个自定义的提示和分析器有LangChain表达的语言(LCEL)，使用普通功能分析的输出模型："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0fb11ba8df44274"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person.\"\"\"\n",
    "\n",
    "    name: str = Field(..., description=\"The name of the person\")\n",
    "    height_in_meters: float = Field(\n",
    "        ..., description=\"The height of the person expressed in meters.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class People(BaseModel):\n",
    "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
    "\n",
    "    people: List[Person]\n",
    "\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user query. Output your answer as JSON that  \"\n",
    "            \"matches the given schema: ```json\\n{schema}\\n```. \"\n",
    "            \"Make sure to wrap the answer in ```json and ``` tags\",\n",
    "        ),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ").partial(schema=People.schema())\n",
    "\n",
    "\n",
    "# Custom parser\n",
    "def extract_json(message: AIMessage) -> List[dict]:\n",
    "    \"\"\"Extracts JSON content from a string where JSON is embedded between ```json and ``` tags.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text containing the JSON content.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of extracted JSON strings.\n",
    "    \"\"\"\n",
    "    text = message.content\n",
    "    # Define the regular expression pattern to match JSON blocks\n",
    "    pattern = r\"```json(.*?)```\"\n",
    "\n",
    "    # Find all non-overlapping matches of the pattern in the string\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "\n",
    "    # Return the list of matched JSON strings, stripping any leading or trailing whitespace\n",
    "    try:\n",
    "        return [json.loads(match.strip()) for match in matches]\n",
    "    except Exception:\n",
    "        raise ValueError(f\"Failed to parse: {message}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T02:08:23.412483Z",
     "start_time": "2024-07-19T02:08:23.406589Z"
    }
   },
   "id": "431ed07414000cde",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: Answer the user query. Output your answer as JSON that  matches the given schema: ```json\n",
      "{'title': 'People', 'description': 'Identifying information about all people in a text.', 'type': 'object', 'properties': {'people': {'title': 'People', 'type': 'array', 'items': {'$ref': '#/definitions/Person'}}}, 'required': ['people'], 'definitions': {'Person': {'title': 'Person', 'description': 'Information about a person.', 'type': 'object', 'properties': {'name': {'title': 'Name', 'description': 'The name of the person', 'type': 'string'}, 'height_in_meters': {'title': 'Height In Meters', 'description': 'The height of the person expressed in meters.', 'type': 'number'}}, 'required': ['name', 'height_in_meters']}}}\n",
      "```. Make sure to wrap the answer in ```json and ``` tags\n",
      "Human: Anna is 23 years old and she is 6 feet tall\n"
     ]
    }
   ],
   "source": [
    "query = \"Anna is 23 years old and she is 6 feet tall\"\n",
    "\n",
    "print(prompt.format_prompt(query=query).to_string())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T02:08:24.051060Z",
     "start_time": "2024-07-19T02:08:24.038534Z"
    }
   },
   "id": "df94781e6ff35fe3",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | qw_llm_openai | extract_json\n",
    "\n",
    "chain.invoke({\"query\": query})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-19T02:08:50.855567Z",
     "start_time": "2024-07-19T02:08:44.482595Z"
    }
   },
   "id": "ccfe45d6ffd82ac9",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2a72b11adcdfb0cc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
