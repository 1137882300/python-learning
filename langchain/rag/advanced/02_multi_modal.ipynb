{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 多模态-rag"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67aee36242fbb718"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Advanced RAG - 02. Multi-Modal RAG\n",
    "The previous tutorial 01. RAG on Semi-structured data introduced RAG development on semi-structured data, for example texts and tables in PDF documents.\n",
    "\n",
    "BUT, it still can't read images.\n",
    "\n",
    "Let's learn how to enable image recognition in RAG by employing multi-modal models.\n",
    "\n",
    "What is Multi-Modal model?\n",
    "\n",
    "Multi-Modal model can process and analyze data from multiple modalities and provide a more complete and accurate understanding of the underlying data.\n",
    "\n",
    "GPT-4V\n",
    "\n",
    "GPT-4V is a multi-modal model that takes in both text and images, and responds with text output. Please refer to GPT-4 Vision for introduction and API guide.\n",
    "\n",
    "In this tutorial, let's use GPT-4V model to implement multi-modal RAG application that can understand the images embedded in the PDF document and answer relevant questions.\n",
    "\n",
    "The PDF document we use in this example is the JP Morgan - Weekly Market Recap. It's a small PDF file containing several tables which is a good example for quick data processing and clear demonstration.\n",
    "\n",
    "Prepare Environment\n",
    "Let's install the necessary Python packages."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5817dbb94b7facc4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install langchain unstructured[all-docs] pydantic lxml openai chromadb tiktoken -q -U\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# !apt-get install poppler-utils tesseract-ocr\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23c781628c0e5588"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "from typing import Any\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "images_path = \"./images\"\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=\"weekly_market_recap.pdf\",\n",
    "    extract_images_in_pdf=True,\n",
    "    infer_table_structure=True,\n",
    "    chunking_strategy=\"by_title\",\n",
    "    max_characters=4000,\n",
    "    new_after_n_chars=3800,\n",
    "    combine_text_under_n_chars=2000,\n",
    "    image_output_dir_path=images_path,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "930fcafcf9a56d1d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image('images/figure-1-1.jpg')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdbc836caea79f44"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Image summarizer\n",
    "\n",
    "import base64\n",
    "import os\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.messages import HumanMessage\n",
    "\n",
    "\n",
    "class ImageSummarizer:\n",
    "\n",
    "    def __init__(self, image_path) -> None:\n",
    "        self.image_path = image_path\n",
    "        self.prompt = \"\"\"\n",
    "                    You are an assistant tasked with summarizing images for retrieval.\n",
    "                    These summaries will be embedded and used to retrieve the raw image.\n",
    "                    Give a concise summary of the image that is well optimized for retrieval.\n",
    "                    \"\"\"\n",
    "\n",
    "    def base64_encode_image(self):\n",
    "        with open(self.image_path, \"rb\") as image_file:\n",
    "            return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "    def summarize(self, prompt=None):\n",
    "        base64_image_data = self.base64_encode_image()\n",
    "        chat = ChatOpenAI(model=\"gpt-4-vision-preview\", max_tokens=1000)\n",
    "\n",
    "        # gpt4 vision api doc - https://platform.openai.com/docs/guides/vision\n",
    "        response = chat.invoke(\n",
    "            [\n",
    "                HumanMessage(\n",
    "                    content=[\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": prompt if prompt else self.prompt\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image_data}\"},\n",
    "                        },\n",
    "                    ]\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        return base64_image_data, response.content"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2783fe4a4183cbe"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "image_data_list = []\n",
    "image_summary_list = []\n",
    "\n",
    "for img_file in sorted(os.listdir(images_path)):\n",
    "    if img_file.endswith(\".jpg\"):\n",
    "        summarizer = ImageSummarizer(os.path.join(images_path, img_file))\n",
    "        data, summary = summarizer.summarize()\n",
    "        image_data_list.append(data)\n",
    "        image_summary_list.append(summary)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf9d507da038e2be"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Element(BaseModel):\n",
    "    type: str\n",
    "    text: Any\n",
    "\n",
    "\n",
    "table_elements = []\n",
    "text_elements = []\n",
    "for element in raw_pdf_elements:\n",
    "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "        table_elements.append(Element(type=\"table\", text=str(element)))\n",
    "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "        text_elements.append(Element(type=\"text\", text=str(element)))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e243ea0f16c8a1f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "prompt_text = \"\"\"\n",
    "  You are responsible for concisely summarizing table or text chunk:\n",
    "\n",
    "  {element}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "summarize_chain = {\"element\": lambda x: x} | prompt | ChatOpenAI(temperature=0,\n",
    "                                                                 model=\"gpt-3.5-turbo\") | StrOutputParser()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74db7eabbf310ede"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "tables = [i.text for i in table_elements]\n",
    "table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})\n",
    "\n",
    "texts = [i.text for i in text_elements]\n",
    "text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ba3a567242e3e77"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use LangChain MultiVectorRetriever to associate summaries of tables, texts and images \n",
    "with original data chunks in parent-child relationship.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c2eecfb8d637a9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.schema.document import Document\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever (empty to start)\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=Chroma(collection_name=\"summaries\", embedding_function=OpenAIEmbeddings()),\n",
    "    docstore=InMemoryStore(),\n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "# Add texts\n",
    "doc_ids = [str(uuid.uuid4()) for _ in texts]\n",
    "summary_texts = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(text_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_texts)\n",
    "retriever.docstore.mset(list(zip(doc_ids, texts)))\n",
    "\n",
    "# Add tables\n",
    "table_ids = [str(uuid.uuid4()) for _ in tables]\n",
    "summary_tables = [\n",
    "    Document(page_content=s, metadata={id_key: table_ids[i]})\n",
    "    for i, s in enumerate(table_summaries)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_tables)\n",
    "retriever.docstore.mset(list(zip(table_ids, tables)))\n",
    "\n",
    "# Add images\n",
    "# image_data_list = []\n",
    "# image_summary_list = []\n",
    "doc_ids = [str(uuid.uuid4()) for _ in image_data_list]\n",
    "summary_images = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(image_summary_list)\n",
    "]\n",
    "retriever.vectorstore.add_documents(summary_images)\n",
    "retriever.docstore.mset(list(zip(doc_ids, image_data_list)))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37796a3efadece29"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Image helper functions\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80e292dddb08ea75"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import HTML, display\n",
    "import io\n",
    "import re\n",
    "\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    display(HTML(f''))\n",
    "\n",
    "\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xFF\\xD8\\xFF\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content\n",
    "\n",
    "        if is_image_data(doc):\n",
    "            b64_images.append(doc)\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "    return {\"images\": b64_images, \"texts\": texts}\n",
    "\n",
    "\n",
    "def img_prompt_func(data_dict):\n",
    "    messages = []\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "\n",
    "    # Adding texts to the messages\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are financial analyst.\\n\"\n",
    "            \"You will be given a mixed of text, tables, and image(s) usually of charts or graphs.\\n\"\n",
    "            \"Use this information to answer the user question in the finance. \\n\"\n",
    "            f\"Question: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and / or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    return [HumanMessage(content=messages)]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5544454bbc7063df"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4-vision-preview\", max_tokens=1024)\n",
    "\n",
    "# RAG pipeline\n",
    "chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(img_prompt_func)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28902281bcd9fcb7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "query = \"Which year had the highest holiday sales growth?\"\n",
    "chain.invoke(query)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b32fcee1ce204a8a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "len(docs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "acc757e65aa2801e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "is_image_data(docs[1])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af1ac3da1734046f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt_img_base64(docs[1])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e24586c60c502ac"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
