{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deb39fae97806171",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T10:04:00.147387Z",
     "start_time": "2024-08-08T10:03:59.502625Z"
    }
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from typing import List, Tuple\n",
    "from chromadb import Settings\n",
    "import chromadb\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    format_document,\n",
    ")\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import (\n",
    "    RunnableBranch,\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 基本配置\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "qw_llm_openai = ChatOpenAI(\n",
    "    openai_api_base=os.getenv('DASHSCOPE_API_BASE'),\n",
    "    openai_api_key=os.getenv('DASHSCOPE_API_KEY'),\n",
    "    model_name=\"qwen2-1.5b-instruct\",\n",
    "    temperature=0,\n",
    "    streaming=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T10:04:01.572213Z",
     "start_time": "2024-08-08T10:04:01.263671Z"
    }
   },
   "id": "78a54930db400e8a",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DashScopeEmbeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m qw_embedding \u001B[38;5;241m=\u001B[39m \u001B[43mDashScopeEmbeddings\u001B[49m(\n\u001B[1;32m      2\u001B[0m     model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext-embedding-v2\u001B[39m\u001B[38;5;124m\"\u001B[39m, dashscope_api_key\u001B[38;5;241m=\u001B[39mos\u001B[38;5;241m.\u001B[39mgetenv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDASHSCOPE_API_KEY\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      3\u001B[0m )\n\u001B[1;32m      5\u001B[0m DATA_DIR \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/Users/pangmengting/Documents/workspace/python-learning/data\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      6\u001B[0m CHROMA_DATA_PATH \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mDATA_DIR\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/chroma_vector_db\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'DashScopeEmbeddings' is not defined"
     ]
    }
   ],
   "source": [
    "qw_embedding = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v2\", dashscope_api_key=os.getenv('DASHSCOPE_API_KEY')\n",
    ")\n",
    "\n",
    "DATA_DIR = '/Users/pangmengting/Documents/workspace/python-learning/data'\n",
    "CHROMA_DATA_PATH = f\"{DATA_DIR}/chroma_vector_db\"\n",
    "CHROMA_CLIENT = chromadb.PersistentClient(\n",
    "    path=CHROMA_DATA_PATH,\n",
    "    settings=Settings(allow_reset=True, anonymized_telemetry=False),\n",
    ")\n",
    "\n",
    "fix_collection_name = 'yxk-know-index-3'\n",
    "persist_directory = '/Users/pangmengting/Documents/workspace/python-learning/data/chroma_vector_db'\n",
    "vectordb = Chroma(collection_name=fix_collection_name,\n",
    "                  client=CHROMA_CLIENT,\n",
    "                  embedding_function=qw_embedding)\n",
    "\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "# Condense a chat history and follow-up question into a standalone question\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"  # noqa: E501\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "# RAG answer synthesis prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", template),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Conversational Retrieval Chain\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "\n",
    "def _combine_documents(docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "\n",
    "def _format_chat_history(chat_history: List[Tuple[str, str]]) -> List:\n",
    "    buffer = []\n",
    "    for human, ai in chat_history:\n",
    "        buffer.append(HumanMessage(content=human))\n",
    "        buffer.append(AIMessage(content=ai))\n",
    "    return buffer\n",
    "\n",
    "\n",
    "# User input\n",
    "class ChatHistory(BaseModel):\n",
    "    chat_history: List[Tuple[str, str]] = Field(..., extra={\"widget\": {\"type\": \"chat\"}})\n",
    "    question: str\n",
    "\n",
    "\n",
    "# 一个基于条件运行两个分支之一的可运行对象。\n",
    "_search_query = RunnableBranch(\n",
    "    # If input includes chat_history, we condense it with the follow-up question\n",
    "    # 如果输入包含聊天记录，我们将其与后续问题一起压缩\n",
    "    (\n",
    "        # 它接受一个参数 x，并检查 x 中是否存在键 \"chat_history\"。如果存在，它返回 True，否则返回 False。\n",
    "        # .with_config(run_name=\"HasChatHistoryCheck\")这是一个方法调用，\n",
    "        # 配置 RunnableLambda 对象的运行名称为 \"HasChatHistoryCheck\"。配置名称通常用于跟踪或调试任务执行。\n",
    "        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
    "            run_name=\"HasChatHistoryCheck\"\n",
    "        ),  # Condense follow-up question and chat into a standalone_question 将后续问题和聊天精简为一个独立的问题 \n",
    "        # .assign(...) 是一个方法，用于创建新的键值对或修改现有的键值对。\n",
    "        RunnablePassthrough.assign(\n",
    "            # chat_history 键的值替换为 _format_chat_history(x[\"chat_history\"]) 的结果\n",
    "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "        )\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        | qw_llm_openai\n",
    "        | StrOutputParser(),\n",
    "    ),\n",
    "    # Else, we have no chat history, so just pass through the question\n",
    "    # 使用 RunnableLambda 类来包装一个函数（在本例中是 itemgetter(\"question\")），使其成为一个可执行对象\n",
    "    # 这在数据流处理或管道处理中非常有用，尤其是在需要从复杂的数据结构中提取特定信息的场景中。\n",
    "    RunnableLambda(itemgetter(\"question\")),\n",
    ")\n",
    "# RunnableParallel 是一个类，用于并行处理多个输入任务，并将这些任务的结果组合在一起。\n",
    "# 这可以用来处理多个不同的操作，并将它们的输出以键值对的形式返回。让我们分解代码并详细解释其中的各个部分。\n",
    "# 这些任务会并行执行，然后将它们的结果组合在一起。\n",
    "# 提取并返回 \"question\"。\n",
    "# 格式化并返回 \"chat_history\"。\n",
    "# 通过一系列操作生成并返回 \"context\"。\n",
    "# 最终，这些结果将作为并行执行的输出，返回一个包含 {\"question\": ..., \"chat_history\": ..., \"context\": ...} 这样的字典。\n",
    "_inputs = RunnableParallel(\n",
    "    {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: _format_chat_history(x[\"chat_history\"]),\n",
    "        \"context\": _search_query | retriever | _combine_documents,\n",
    "    }\n",
    ").with_types(input_type=ChatHistory)\n",
    "\n",
    "chain = _inputs | ANSWER_PROMPT | qw_llm_openai | StrOutputParser()"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-09T01:30:38.830818Z",
     "start_time": "2024-08-09T01:30:38.731541Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "question = \"你是谁啊\"\n",
    "answer = chain.invoke({\"question\": question, \"chat_history\": []})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T10:52:21.342099Z",
     "start_time": "2024-08-08T10:52:08.831525Z"
    }
   },
   "id": "580b940d78699d08",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'我是阿里云开发的语言模型，我叫通义千问。'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T10:52:37.119412Z",
     "start_time": "2024-08-08T10:52:37.115052Z"
    }
   },
   "id": "3373e2680852b69",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'我叫通义千问，是由阿里云开发的大型预训练语言模型。'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = [(question, answer)]\n",
    "answer = chain.invoke(\n",
    "    {\n",
    "        \"question\": \"叫什么?\",\n",
    "        \"chat_history\": chat_history,\n",
    "    }\n",
    ")\n",
    "answer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T10:53:08.878009Z",
     "start_time": "2024-08-08T10:52:55.718839Z"
    }
   },
   "id": "26427861bd5fa8d0",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4e7a183d6dfb5552"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
