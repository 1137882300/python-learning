{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deb39fae97806171",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T10:04:00.147387Z",
     "start_time": "2024-08-08T10:03:59.502625Z"
    }
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from typing import List, Tuple\n",
    "from chromadb import Settings\n",
    "import chromadb\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    format_document,\n",
    ")\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import (\n",
    "    RunnableBranch,\n",
    "    RunnableLambda,\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 基本配置\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "qw_llm_openai = ChatOpenAI(\n",
    "    openai_api_base=os.getenv('DASHSCOPE_API_BASE'),\n",
    "    openai_api_key=os.getenv('DASHSCOPE_API_KEY'),\n",
    "    model_name=\"qwen2-1.5b-instruct\",\n",
    "    temperature=0,\n",
    "    streaming=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T10:04:01.572213Z",
     "start_time": "2024-08-08T10:04:01.263671Z"
    }
   },
   "id": "78a54930db400e8a",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "qw_embedding = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v2\", dashscope_api_key=os.getenv('DASHSCOPE_API_KEY')\n",
    ")\n",
    "\n",
    "DATA_DIR = '/Users/pangmengting/Documents/workspace/python-learning/data'\n",
    "CHROMA_DATA_PATH = f\"{DATA_DIR}/chroma_vector_db\"\n",
    "CHROMA_CLIENT = chromadb.PersistentClient(\n",
    "    path=CHROMA_DATA_PATH,\n",
    "    settings=Settings(allow_reset=True, anonymized_telemetry=False),\n",
    ")\n",
    "\n",
    "fix_collection_name = 'yxk-know-index-3'\n",
    "persist_directory = '/Users/pangmengting/Documents/workspace/python-learning/data/chroma_vector_db'\n",
    "vectordb = Chroma(collection_name=fix_collection_name,\n",
    "                  client=CHROMA_CLIENT,\n",
    "                  embedding_function=qw_embedding)\n",
    "\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "# Condense a chat history and follow-up question into a standalone question\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"  # noqa: E501\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "# RAG answer synthesis prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", template),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Conversational Retrieval Chain\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "\n",
    "def _combine_documents(docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "\n",
    "def _format_chat_history(chat_history: List[Tuple[str, str]]) -> List:\n",
    "    buffer = []\n",
    "    for human, ai in chat_history:\n",
    "        buffer.append(HumanMessage(content=human))\n",
    "        buffer.append(AIMessage(content=ai))\n",
    "    return buffer\n",
    "\n",
    "\n",
    "# User input\n",
    "class ChatHistory(BaseModel):\n",
    "    chat_history: List[Tuple[str, str]] = Field(..., extra={\"widget\": {\"type\": \"chat\"}})\n",
    "    question: str\n",
    "\n",
    "\n",
    "_search_query = RunnableBranch(\n",
    "    # If input includes chat_history, we condense it with the follow-up question\n",
    "    (\n",
    "        RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\n",
    "            run_name=\"HasChatHistoryCheck\"\n",
    "        ),  # Condense follow-up question and chat into a standalone_question\n",
    "        RunnablePassthrough.assign(\n",
    "            chat_history=lambda x: _format_chat_history(x[\"chat_history\"])\n",
    "        )\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        | qw_llm_openai\n",
    "        | StrOutputParser(),\n",
    "    ),\n",
    "    # Else, we have no chat history, so just pass through the question\n",
    "    RunnableLambda(itemgetter(\"question\")),\n",
    ")\n",
    "\n",
    "_inputs = RunnableParallel(\n",
    "    {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        # \"question\": lambda x: x[\"question\"] if isinstance(x, dict) else x,\n",
    "        \"chat_history\": lambda x: _format_chat_history(x[\"chat_history\"]),\n",
    "        # \"chat_history\": lambda x: _format_chat_history(x[\"chat_history\"] if isinstance(x, dict) else x),\n",
    "        \"context\": _search_query | retriever | _combine_documents,\n",
    "    }\n",
    ").with_types(input_type=ChatHistory)\n",
    "\n",
    "chain = _inputs | ANSWER_PROMPT | qw_llm_openai | StrOutputParser()"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-08T10:46:13.162435Z",
     "start_time": "2024-08-08T10:46:13.127301Z"
    }
   },
   "id": "initial_id",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "question = \"你是谁啊\"\n",
    "answer = chain.invoke({\"question\": question, \"chat_history\": []})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T10:52:21.342099Z",
     "start_time": "2024-08-08T10:52:08.831525Z"
    }
   },
   "id": "580b940d78699d08",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'我是阿里云开发的语言模型，我叫通义千问。'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T10:52:37.119412Z",
     "start_time": "2024-08-08T10:52:37.115052Z"
    }
   },
   "id": "3373e2680852b69",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'我叫通义千问，是由阿里云开发的大型预训练语言模型。'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = [(question, answer)]\n",
    "answer = chain.invoke(\n",
    "    {\n",
    "        \"question\": \"叫什么?\",\n",
    "        \"chat_history\": chat_history,\n",
    "    }\n",
    ")\n",
    "answer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-08T10:53:08.878009Z",
     "start_time": "2024-08-08T10:52:55.718839Z"
    }
   },
   "id": "26427861bd5fa8d0",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4e7a183d6dfb5552"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
