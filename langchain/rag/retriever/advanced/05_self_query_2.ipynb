{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "# Used to condense a question and chat history into a single question\n",
    "condense_question_prompt_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language. If there is no chat history, just rephrase the question to be a standalone question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "\"\"\"  # noqa: E501\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(\n",
    "    condense_question_prompt_template\n",
    ")\n",
    "\n",
    "# RAG Prompt to provide the context and question for LLM to answer\n",
    "# We also ask the LLM to cite the source of the passage it is answering from\n",
    "llm_context_prompt_template = \"\"\"\n",
    "Use the following passages to answer the user's question.\n",
    "Each passage has a SOURCE which is the title of the document. When answering, cite source name of the passages you are answering from below the answer in a unique bullet point list.\n",
    "\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "----\n",
    "{context}\n",
    "----\n",
    "Question: {question}\n",
    "\"\"\"  # noqa: E501\n",
    "\n",
    "LLM_CONTEXT_PROMPT = ChatPromptTemplate.from_template(llm_context_prompt_template)\n",
    "\n",
    "# Used to build a context window from passages retrieved\n",
    "document_prompt_template = \"\"\"\n",
    "---\n",
    "NAME: {name}\n",
    "PASSAGE:\n",
    "{page_content}\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "DOCUMENT_PROMPT = PromptTemplate.from_template(document_prompt_template)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fe0a3af66de2d6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from operator import itemgetter\n",
    "from typing import List, Tuple\n",
    "\n",
    "from langchain.retrievers import SelfQueryRetriever\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import format_document\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_elasticsearch.vectorstores import ElasticsearchStore\n",
    "\n",
    "# from .prompts import CONDENSE_QUESTION_PROMPT, DOCUMENT_PROMPT, LLM_CONTEXT_PROMPT\n",
    "\n",
    "ELASTIC_CLOUD_ID = os.getenv(\"ELASTIC_CLOUD_ID\")\n",
    "ELASTIC_USERNAME = os.getenv(\"ELASTIC_USERNAME\", \"elastic\")\n",
    "ELASTIC_PASSWORD = os.getenv(\"ELASTIC_PASSWORD\")\n",
    "ES_URL = os.getenv(\"ES_URL\", \"http://localhost:9200\")\n",
    "ELASTIC_INDEX_NAME = os.getenv(\"ELASTIC_INDEX_NAME\", \"workspace-search-example\")\n",
    "\n",
    "if ELASTIC_CLOUD_ID and ELASTIC_USERNAME and ELASTIC_PASSWORD:\n",
    "    es_connection_details = {\n",
    "        \"es_cloud_id\": ELASTIC_CLOUD_ID,\n",
    "        \"es_user\": ELASTIC_USERNAME,\n",
    "        \"es_password\": ELASTIC_PASSWORD,\n",
    "    }\n",
    "else:\n",
    "    es_connection_details = {\"es_url\": ES_URL}\n",
    "\n",
    "vecstore = ElasticsearchStore(\n",
    "    ELASTIC_INDEX_NAME,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    **es_connection_details,\n",
    ")\n",
    "# 🔤 中文: 工作场所政策的目的和规格。\n",
    "document_contents = \"The purpose and specifications of a workplace policy.\"\n",
    "metadata_field_info = [\n",
    "    {\"name\": \"name\", \"type\": \"string\", \"description\": \"Name of the workplace policy.\"},\n",
    "    {\n",
    "        \"name\": \"created_on\",\n",
    "        \"type\": \"date\",\n",
    "        \"description\": \"The date the policy was created in ISO 8601 date format (YYYY-MM-DD).\",  # noqa: E501\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"updated_at\",\n",
    "        \"type\": \"date\",\n",
    "        \"description\": \"The date the policy was last updated in ISO 8601 date format (YYYY-MM-DD).\",  # noqa: E501\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"location\",\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"Where the policy text is stored. The only valid values are ['github', 'sharepoint'].\",\n",
    "        # noqa: E501\n",
    "    },\n",
    "]\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm, vecstore, document_contents, metadata_field_info\n",
    ")\n",
    "\n",
    "\n",
    "def _combine_documents(docs: List) -> str:\n",
    "    return \"\\n\\n\".join(format_document(doc, prompt=DOCUMENT_PROMPT) for doc in docs)\n",
    "\n",
    "\n",
    "def _format_chat_history(chat_history: List[Tuple]) -> str:\n",
    "    return \"\\n\".join(f\"Human: {human}\\nAssistant: {ai}\" for human, ai in chat_history)\n",
    "\n",
    "\n",
    "class InputType(BaseModel):\n",
    "    question: str\n",
    "    chat_history: List[Tuple[str, str]] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "standalone_question = (\n",
    "        {\n",
    "            # 获取用户问题的参数，并做简单处理\n",
    "            \"question\": itemgetter(\"question\"),\n",
    "            \"chat_history\": lambda x: _format_chat_history(x[\"chat_history\"]),\n",
    "        }\n",
    "        # 将这两个字段组成的字典，传递给prompt，格式化成 prompt\n",
    "        | CONDENSE_QUESTION_PROMPT\n",
    "        # 把prompt交给llm作为输入\n",
    "        | llm\n",
    "        # 最后把llm的输入格式化\n",
    "        | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n",
    "def route_question(input):\n",
    "    if input.get(\"chat_history\"):\n",
    "        return standalone_question\n",
    "    else:\n",
    "        return RunnablePassthrough()\n",
    "\n",
    "\n",
    "## 先执行retriever，输入是问题，输出是文档list\n",
    "## 再执行_combine_documents，输入是文档list，输出是str\n",
    "## 再执行LLM_CONTEXT_PROMPT，输入 \n",
    "# 添加额外参数，生成字典\n",
    "_context = RunnableParallel(\n",
    "    context=retriever | _combine_documents,\n",
    "    question=RunnablePassthrough(),\n",
    ")\n",
    "\n",
    "# standalone_question，作用是判断，是独立问题，还是具有记忆的问题\n",
    "chain = (\n",
    "    # 处理问题并生成新的问题\n",
    "        standalone_question\n",
    "        # 构建prompt所需的字典参数\n",
    "        | _context\n",
    "        # 结合上一步的字典，生成最终的prompt\n",
    "        | LLM_CONTEXT_PROMPT\n",
    "        # 结合上一步的prompt，生成最终的回答\n",
    "        | llm\n",
    "        # 最后格式化输出\n",
    "        | StrOutputParser()\n",
    ").with_types(input_type=InputType)  # 指定输入的参数类型\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
