{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 压缩"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "796ccfb324522e5e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](img/img_1.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aaada68f89eeaa55"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i + 1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-24T05:53:45.668632Z",
     "start_time": "2024-06-24T05:53:45.664680Z"
    }
   },
   "id": "922574641174f287",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: https://help.aliyun.com/zh/dashscope/developer-reference/tongyi-qianwen-7b-14b-72b-api-detailes?spm=5176.28197632.0.0.11867e06vodKWB\r\n",
      "zsh:1: command not found: -O\r\n"
     ]
    }
   ],
   "source": [
    "# !mkdir data\n",
    "!wget https://raw.githubusercontent.com/run-llama/llama_index/main/README.md -O ../../file/paul_graham_essay.txt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-24T05:53:23.131844Z",
     "start_time": "2024-06-24T05:53:22.981382Z"
    }
   },
   "id": "80f1fa3e7c11e1d0",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/bin/wget\r\n"
     ]
    }
   ],
   "source": [
    "# !which wget\n",
    "# !brew install wget"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-24T05:42:40.975234Z",
     "start_time": "2024-06-24T05:42:40.846760Z"
    }
   },
   "id": "717c9a82a2ce6c7f",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "question = \"Qwen2 是什么?\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-24T05:59:54.681186Z",
     "start_time": "2024-06-24T05:59:54.677742Z"
    }
   },
   "id": "12acef8608f4ae25",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8483c3ec7a0cbc54a8d660b5b9002b04\n",
      "Gcllof8ze6dgtcqFI5FQZ2SD_5tfCD4Db7NuS6jn\n"
     ]
    }
   ],
   "source": [
    "# CloudflareWorkersAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_community.llms.cloudflare_workersai import CloudflareWorkersAI\n",
    "\n",
    "# 加载当前目录下的.env文件\n",
    "# load_dotenv()\n",
    "# load_dotenv(override=True) 会重新读取.env\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# 现在可以像访问普通环境变量一样访问.env文件中的变量了\n",
    "account_id = os.getenv('CF_ACCOUNT_ID')\n",
    "api_token = os.getenv('CF_API_TOKEN')\n",
    "\n",
    "print(account_id)\n",
    "print(api_token)\n",
    "\n",
    "import getpass\n",
    "\n",
    "model = '@cf/meta/llama-3-8b-instruct'\n",
    "cf_llm = CloudflareWorkersAI(account_id=account_id, api_token=api_token, model=model)\n",
    "\n",
    "# 最新的Embedding方式\n",
    "# cloudflare_workersai\n",
    "from langchain_community.embeddings.cloudflare_workersai import (\n",
    "    CloudflareWorkersAIEmbeddings,\n",
    ")\n",
    "\n",
    "# //维度是：384\n",
    "embeddings = CloudflareWorkersAIEmbeddings(\n",
    "    account_id=account_id,\n",
    "    api_token=api_token,\n",
    "    model_name=\"@cf/baai/bge-small-en-v1.5\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-24T05:54:29.812981Z",
     "start_time": "2024-06-24T05:54:29.721345Z"
    }
   },
   "id": "dff5b6bdce14ed65",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Load blog post\n",
    "loader = WebBaseLoader(\n",
    "    \"https://help.aliyun.com/zh/dashscope/developer-reference/tongyi-qianwen-7b-14b-72b-api-detailes?spm=5176.28197632.0.0.11867e06vodKWB&disableWebsiteRedirect=true\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(data)\n",
    "\n",
    "retriever = Chroma.from_documents(splits, embeddings, collection_name=\"example\").as_retriever()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-24T06:00:03.482319Z",
     "start_time": "2024-06-24T05:59:59.055023Z"
    }
   },
   "id": "c6217691c7681d75",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "大语言模型说明 支持的领域 / 任务：aigc Qwen2Qwen2是Qwen开源大语言模型的新系列。参数范围包括0.5B到72B，包括Mixture-of-Experts模型。与最先进的开源语言模型（包括之前发布的 Qwen1.5）相比，Qwen2在一系列针对语言理解、语言生成、多语言能力、编码、数学、推理等的基准测试中总体上超越了大多数开源模型，并表现出与专有模型的竞争力。Qwen2增⼤了上下⽂⻓度⽀持，最⾼达到128K tokens（Qwen2-72B-Instruct），能够处理大量输入。灵积平台上基于Qwen2开源的0.5B、1.5B、7B、72B和57B-A14B MoE模型的instruct版本，并进行了针对性的推理性能优化，为广大开发者提供便捷的API服务。各个版本均对应魔搭社区开源的各个模型版本，详细参考\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "大语言模型说明 支持的领域 / 任务：aigc Qwen2Qwen2是Qwen开源大语言模型的新系列。参数范围包括0.5B到72B，包括Mixture-of-Experts模型。与最先进的开源语言模型（包括之前发布的 Qwen1.5）相比，Qwen2在一系列针对语言理解、语言生成、多语言能力、编码、数学、推理等的基准测试中总体上超越了大多数开源模型，并表现出与专有模型的竞争力。Qwen2增⼤了上下⽂⻓度⽀持，最⾼达到128K tokens（Qwen2-72B-Instruct），能够处理大量输入。灵积平台上基于Qwen2开源的0.5B、1.5B、7B、72B和57B-A14B MoE模型的instruct版本，并进行了针对性的推理性能优化，为广大开发者提供便捷的API服务。各个版本均对应魔搭社区开源的各个模型版本，详细参考\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "2,000。qwen1.5-0.5b-chat通义千问1.5对外开源的0.5B规模参数量的经过人类指令对齐的chat模型codeqwen1.5-7b-chat通义千问1.5对外开源的7B规模参数量的经过人类指令对齐的针对代码场景的chat模型模型支持 64,000 tokens上下文，为了保障正常使用和正常输出，API限定用户输入为56,000，输出最大 8,000。qwen-72b-chat通义千问对外开源的72B规模参数量的经过人类指令对齐的chat模型模型支持 32,000 tokens上下文，为了保障正常使用和正常输出，API限定用户输入为30,000，输出最大 2,000。qwen-14b-chat通义千问对外开源的14B规模参数量的经过人类指令对齐的chat模型模型支持 8,000 tokens上下文，为了保障正常使用和正常输出，API限定用户输入为6,000，输出最大\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "2,000。qwen1.5-0.5b-chat通义千问1.5对外开源的0.5B规模参数量的经过人类指令对齐的chat模型codeqwen1.5-7b-chat通义千问1.5对外开源的7B规模参数量的经过人类指令对齐的针对代码场景的chat模型模型支持 64,000 tokens上下文，为了保障正常使用和正常输出，API限定用户输入为56,000，输出最大 8,000。qwen-72b-chat通义千问对外开源的72B规模参数量的经过人类指令对齐的chat模型模型支持 32,000 tokens上下文，为了保障正常使用和正常输出，API限定用户输入为30,000，输出最大 2,000。qwen-14b-chat通义千问对外开源的14B规模参数量的经过人类指令对齐的chat模型模型支持 8,000 tokens上下文，为了保障正常使用和正常输出，API限定用户输入为6,000，输出最大\n"
     ]
    }
   ],
   "source": [
    "# 普通检索，未压缩\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(docs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-24T06:00:07.428326Z",
     "start_time": "2024-06-24T06:00:06.412880Z"
    }
   },
   "id": "38d18815097656fa",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "Extracted relevant parts:\n",
      "```\n",
      "Qwen2是Qwen开源大语言模型的新系列。参数范围包括0.5B到72B，包括Mixture-of-Experts模型。\n",
      "```\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "Qwen2 是什么?\n",
      "\n",
      "Extracted relevant parts: Qwen2 是 Qwen 开源大语言模型的新系列。\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "NO_OUTPUT\n",
      "\n",
      "The provided question is \"Qwen2 是什么?\" and the context does not mention anything about \"Qwen2\". Therefore, none of the context is relevant to answer the question.\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "# 压缩方式1\n",
    "# LLMChainExtractor：文档压缩器，可以使用LLM链提取文档中的相关部分。\n",
    "compressor = LLMChainExtractor.from_llm(cf_llm)\n",
    "# ContextualCompressionRetriever：包装基础检索器并压缩结果的检索器。\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)\n",
    "\n",
    "# 压缩之后\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(compressed_docs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-24T06:00:35.241680Z",
     "start_time": "2024-06-24T06:00:21.530038Z"
    }
   },
   "id": "c65a04670dc5c100",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "大语言模型说明 支持的领域 / 任务：aigc Qwen2Qwen2是Qwen开源大语言模型的新系列。参数范围包括0.5B到72B，包括Mixture-of-Experts模型。与最先进的开源语言模型（包括之前发布的 Qwen1.5）相比，Qwen2在一系列针对语言理解、语言生成、多语言能力、编码、数学、推理等的基准测试中总体上超越了大多数开源模型，并表现出与专有模型的竞争力。Qwen2增⼤了上下⽂⻓度⽀持，最⾼达到128K tokens（Qwen2-72B-Instruct），能够处理大量输入。灵积平台上基于Qwen2开源的0.5B、1.5B、7B、72B和57B-A14B MoE模型的instruct版本，并进行了针对性的推理性能优化，为广大开发者提供便捷的API服务。各个版本均对应魔搭社区开源的各个模型版本，详细参考\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "大语言模型说明 支持的领域 / 任务：aigc Qwen2Qwen2是Qwen开源大语言模型的新系列。参数范围包括0.5B到72B，包括Mixture-of-Experts模型。与最先进的开源语言模型（包括之前发布的 Qwen1.5）相比，Qwen2在一系列针对语言理解、语言生成、多语言能力、编码、数学、推理等的基准测试中总体上超越了大多数开源模型，并表现出与专有模型的竞争力。Qwen2增⼤了上下⽂⻓度⽀持，最⾼达到128K tokens（Qwen2-72B-Instruct），能够处理大量输入。灵积平台上基于Qwen2开源的0.5B、1.5B、7B、72B和57B-A14B MoE模型的instruct版本，并进行了针对性的推理性能优化，为广大开发者提供便捷的API服务。各个版本均对应魔搭社区开源的各个模型版本，详细参考\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "2,000。qwen1.5-0.5b-chat通义千问1.5对外开源的0.5B规模参数量的经过人类指令对齐的chat模型codeqwen1.5-7b-chat通义千问1.5对外开源的7B规模参数量的经过人类指令对齐的针对代码场景的chat模型模型支持 64,000 tokens上下文，为了保障正常使用和正常输出，API限定用户输入为56,000，输出最大 8,000。qwen-72b-chat通义千问对外开源的72B规模参数量的经过人类指令对齐的chat模型模型支持 32,000 tokens上下文，为了保障正常使用和正常输出，API限定用户输入为30,000，输出最大 2,000。qwen-14b-chat通义千问对外开源的14B规模参数量的经过人类指令对齐的chat模型模型支持 8,000 tokens上下文，为了保障正常使用和正常输出，API限定用户输入为6,000，输出最大\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.document_compressors import LLMChainFilter\n",
    "\n",
    "# 压缩方式2\n",
    "# LLMChainFilter：过滤器，删除与查询无关的文档。\n",
    "_filter = LLMChainFilter.from_llm(cf_llm)\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=_filter, base_retriever=retriever)\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(compressed_docs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-24T06:03:42.207736Z",
     "start_time": "2024-06-24T06:03:35.699921Z"
    }
   },
   "id": "7aa72b42f3084b8b",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "\n",
    "# 压缩方式3\n",
    "# EmbeddingsFilter: 一种使用嵌入技术来丢弃与查询无关的文档的文档压缩器。\n",
    "embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.8)\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=embeddings_filter, base_retriever=retriever)\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(compressed_docs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-24T06:12:28.845518Z",
     "start_time": "2024-06-24T06:12:25.658710Z"
    }
   },
   "id": "b90daa674b753871",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.document_transformers import EmbeddingsRedundantFilter\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 压缩方式4\n",
    "splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0, separator=\". \")\n",
    "# EmbeddingsRedundantFilter：通过比较其嵌入来丢弃冗余文档的过滤器\n",
    "redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n",
    "relevant_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76)\n",
    "# DocumentCompressorPipeline： 使用 Transformer 管道的文档压缩器。\n",
    "pipeline_compressor = DocumentCompressorPipeline(\n",
    "    transformers=[splitter, redundant_filter, relevant_filter]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-24T06:16:42.588232Z",
     "start_time": "2024-06-24T06:16:42.564612Z"
    }
   },
   "id": "b821a6c4b550d989",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "大语言模型说明 支持的领域 / 任务：aigc Qwen2Qwen2是Qwen开源大语言模型的新系列。参数范围包括0.5B到72B，包括Mixture-of-Experts模型。与最先进的开源语言模型（包括之前发布的 Qwen1.5）相比，Qwen2在一系列针对语言理解、语言生成、多语言能力、编码、数学、推理等的基准测试中总体上超越了大多数开源模型，并表现出与专有模型的竞争力。Qwen2增⼤了上下⽂⻓度⽀持，最⾼达到128K tokens（Qwen2-72B-Instruct），能够处理大量输入。灵积平台上基于Qwen2开源的0.5B、1.5B、7B、72B和57B-A14B MoE模型的instruct版本，并进行了针对性的推理性能优化，为广大开发者提供便捷的API服务。各个版本均对应魔搭社区开源的各个模型版本，详细参考\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "2,000。qwen1.5-0.5b-chat通义千问1.5对外开源的0.5B规模参数量的经过人类指令对齐的chat模型codeqwen1.5-7b-chat通义千问1.5对外开源的7B规模参数量的经过人类指令对齐的针对代码场景的chat模型模型支持 64,000 tokens上下文，为了保障正常使用和正常输出，API限定用户输入为56,000，输出最大 8,000。qwen-72b-chat通义千问对外开源的72B规模参数量的经过人类指令对齐的chat模型模型支持 32,000 tokens上下文，为了保障正常使用和正常输出，API限定用户输入为30,000，输出最大 2,000。qwen-14b-chat通义千问对外开源的14B规模参数量的经过人类指令对齐的chat模型模型支持 8,000 tokens上下文，为了保障正常使用和正常输出，API限定用户输入为6,000，输出最大\n"
     ]
    }
   ],
   "source": [
    "# 压缩方式5\n",
    "# ContextualCompressionRetriever：包装基础检索器并压缩结果的检索器。\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=pipeline_compressor, base_retriever=retriever)\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(compressed_docs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-24T06:18:05.584388Z",
     "start_time": "2024-06-24T06:18:02.451977Z"
    }
   },
   "id": "6d312602239446fc",
   "execution_count": 27
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
