{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-09T01:25:21.862865Z",
     "start_time": "2024-07-09T01:25:21.385251Z"
    }
   },
   "outputs": [],
   "source": [
    "# 基本配置\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "qw_llm_openai = ChatOpenAI(\n",
    "    openai_api_base=os.getenv('DASHSCOPE_API_BASE'),\n",
    "    openai_api_key=os.getenv('DASHSCOPE_API_KEY'),\n",
    "    model_name=\"qwen2-1.5b-instruct\",\n",
    "    temperature=0,\n",
    "    streaming=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.cloudflare_workersai import CloudflareWorkersAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "embedding = CloudflareWorkersAIEmbeddings(\n",
    "    account_id=os.getenv('CF_ACCOUNT_ID'),\n",
    "    api_token=os.getenv('CF_API_TOKEN'),\n",
    "    model_name=\"@cf/baai/bge-small-en-v1.5\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T01:25:53.058099Z",
     "start_time": "2024-07-09T01:25:53.019150Z"
    }
   },
   "id": "38ace70da450d07b",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "import bs4\n",
    "# from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1. Load, chunk and index the contents of the blog to create a retriever.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embedding)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 2. Incorporate the retriever into a question-answering chain.\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    "    \"\\n\\n\"\n",
    "    \"Previous conversation:\\n{history}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(qw_llm_openai, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T01:26:28.368057Z",
     "start_time": "2024-07-09T01:26:22.642860Z"
    }
   },
   "id": "4f4e31102b1bbbdd",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7e86ea465044522"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## rag + llm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c114687e5810f8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from typing import Optional, Dict\n",
    "from langchain_core.runnables.utils import Input\n",
    "from langchain_core.runnables import Runnable, RunnableConfig\n",
    "from langchain_core.load import Serializable\n",
    "\n",
    "\n",
    "# 自定义一个继承Runnable的类\n",
    "class StdOutputRunnable(Serializable, Runnable[Input, Input]):\n",
    "    @property\n",
    "    def lc_serializable(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def invoke(self, input: Dict, config: Optional[RunnableConfig] = None) -> Input:\n",
    "        # print(f\"Hey, I received the name {input['name']}\")\n",
    "        print(input)\n",
    "        return self._call_with_config(lambda x: x, input, config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T06:11:49.374500Z",
     "start_time": "2024-07-09T06:11:49.365496Z"
    }
   },
   "id": "d0418405487f640a",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "# 这个函数接收一个文档列表，并将它们的页面内容（page_content）合并成一个单一的字符串，每个文档之间用两个换行符分隔\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# 执行流程2:\n",
    "# rag_chain_from_docs 使用 retrieve_docs 的输出（问题）和 format_docs 函数来格式化检索到的文档内容。\n",
    "rag_chain_from_docs = (\n",
    "    # 这一步接收 retrieve_docs 函数的输出，将其作为 context 传递给 format_docs 函数，生成格式化的文档内容字符串。\n",
    "        RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"retriever_context\"])))\n",
    "        | prompt\n",
    "        | qw_llm_openai\n",
    "        | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 执行流程1: 从字典中提取 input 键的值，即 \"What is Task Decomposition\"。用户的问题\n",
    "# retrieve_docs 函数的输出是一个包含 context 键的字典，那么 x 就会包含这个键\n",
    "retrieve_docs = (lambda x: x[\"input\"]) | retriever\n",
    "# retrieve_docs = (lambda x: {\"retriever_context\": retriever.get_relevant_documents(x[\"input\"])})\n",
    "\n",
    "# 组合\n",
    "# chain：这个变量组合了两个步骤，\n",
    "# 首先通过 retrieve_docs 获取问题，\n",
    "# 然后通过 rag_chain_from_docs 来生成答案。\n",
    "# retriever_context会被传递到下一个步骤里去\n",
    "chain = RunnablePassthrough.assign(retriever_context=retrieve_docs).assign(\n",
    "    answer=rag_chain_from_docs\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T02:35:47.603619Z",
     "start_time": "2024-07-09T02:35:47.599571Z"
    }
   },
   "id": "149c7449ee16086f",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'input': 'What is Task Decomposition',\n 'retriever_context': [Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n  Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n  Document(page_content='The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n  Document(page_content='Fig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})],\n 'answer': 'Task Decomposition is a method used to break down complex tasks into smaller, more manageable ones. It involves breaking the original task into multiple steps or sub-tasks that can be addressed independently. This approach helps in optimizing computational resources and improving the efficiency of solving complex problems.'}"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 详细流程：\n",
    "# 1. 从字典中提取 input 键的值，即 \"What is Task Decomposition\"。用户的问题\n",
    "# 2. 使用 retrieve_docs 函数来检索文档，并把结果赋值给 retriever_context，再传递给下一个步骤（即rag_chain_from_docs）\n",
    "# 3. rag_chain_from_docs 使用 retrieve_docs 的输出（问题）和 format_docs 函数来格式化检索到的文档内容。\n",
    "chain.invoke({\"input\": \"What is Task Decomposition\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T02:29:58.287459Z",
     "start_time": "2024-07-09T02:29:55.340779Z"
    }
   },
   "id": "c133b330361fb8a6",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "## rag + memory + llm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e0786f5d8b215f6"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qw_llm_openai' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 48\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# memory = ConversationBufferMemory(chat_memory=message_history, return_messages=True)\u001B[39;00m\n\u001B[1;32m     38\u001B[0m \n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# memory3\u001B[39;00m\n\u001B[1;32m     40\u001B[0m memory \u001B[38;5;241m=\u001B[39m ConversationBufferWindowMemory(k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, chat_memory\u001B[38;5;241m=\u001B[39mmessage_history, return_messages\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     42\u001B[0m rag_chain_from_docs \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     43\u001B[0m         RunnablePassthrough\u001B[38;5;241m.\u001B[39massign(\n\u001B[1;32m     44\u001B[0m             context\u001B[38;5;241m=\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m x: format_docs(x[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mretriever_context\u001B[39m\u001B[38;5;124m\"\u001B[39m])),\n\u001B[1;32m     45\u001B[0m             history\u001B[38;5;241m=\u001B[39mmemory\u001B[38;5;241m.\u001B[39mload_memory_variables\n\u001B[1;32m     46\u001B[0m         )\n\u001B[1;32m     47\u001B[0m         \u001B[38;5;241m|\u001B[39m prompt\n\u001B[0;32m---> 48\u001B[0m         \u001B[38;5;241m|\u001B[39m \u001B[43mqw_llm_openai\u001B[49m\n\u001B[1;32m     49\u001B[0m         \u001B[38;5;241m|\u001B[39m StrOutputParser()\n\u001B[1;32m     50\u001B[0m )\n\u001B[1;32m     51\u001B[0m retrieve_docs \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28;01mlambda\u001B[39;00m x: x[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput\u001B[39m\u001B[38;5;124m\"\u001B[39m]) \u001B[38;5;241m|\u001B[39m retriever \n\u001B[1;32m     52\u001B[0m \u001B[38;5;66;03m# retrieve_docs = lambda x: {\"retriever_context\": retriever.get_relevant_documents(x[\"input\"])}\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'qw_llm_openai' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "from langchain.memory.chat_message_histories import FileChatMessageHistory\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    "    \"\\n\\n\"\n",
    "    \"Previous conversation:\\n{history}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# memory1\n",
    "# memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "# memory2\n",
    "path = '../../../data/history/conversation_20240709-2.json'\n",
    "message_history = FileChatMessageHistory(file_path=path)\n",
    "# memory = ConversationBufferMemory(chat_memory=message_history, return_messages=True)\n",
    "\n",
    "# memory3\n",
    "memory = ConversationBufferWindowMemory(k=2, chat_memory=message_history, return_messages=True)\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "        RunnablePassthrough.assign(\n",
    "            context=(lambda x: format_docs(x[\"retriever_context\"])),\n",
    "            history=memory.load_memory_variables\n",
    "        )\n",
    "        | prompt\n",
    "        | qw_llm_openai\n",
    "        | StrOutputParser()\n",
    ")\n",
    "retrieve_docs = (lambda x: x[\"input\"]) | retriever \n",
    "# retrieve_docs = lambda x: {\"retriever_context\": retriever.get_relevant_documents(x[\"input\"])}\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(retriever_context=retrieve_docs)\n",
    "    .assign(answer=rag_chain_from_docs)\n",
    "    .assign(\n",
    "        memory_update=lambda x: memory.save_context(\n",
    "            {\"input\": x[\"input\"]},\n",
    "            {\"output\": x[\"answer\"]}\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "def chat(input_text):\n",
    "    result = chain.invoke({\"input\": input_text})\n",
    "    return result[\"answer\"]\n",
    "\n",
    "# 使用方法\n",
    "# response = chat(\"What is Task Decomposition?\")\n",
    "# print(response)\n",
    "# 继续对话\n",
    "# response = chat(\"Can you give me an example?\")\n",
    "# print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T06:11:27.817420Z",
     "start_time": "2024-07-09T06:11:27.499527Z"
    }
   },
   "id": "c40e73a5bc0256ab",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Decomposition is a method used in artificial intelligence where a large task is broken down into smaller, more manageable parts called subtasks. This approach helps the AI system understand the task better and perform it more efficiently.\n"
     ]
    }
   ],
   "source": [
    "response = chat(\"What is Task Decomposition?\")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T03:14:06.313201Z",
     "start_time": "2024-07-09T03:14:03.414414Z"
    }
   },
   "id": "281c1d1e6784e97e",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"thoughts\": {\n",
      "        \"text\": \"The user wants an example of task decomposition.\",\n",
      "        \"reasoning\": \"I will provide an example of how task decomposition works in a simple scenario.\",\n",
      "        \"plan\": \"- Provide an example of task decomposition\\n- Explain how it breaks down the task into smaller parts\\n- Suggest that the user provides feedback on whether they understood the concept\",\n",
      "        \"criticism\": \"It's important to ensure the explanation is clear and understandable.\",\n",
      "        \"speak\": \"Sure, here's an example: Imagine you're organizing a party. You have a list of tasks to complete: buy decorations, invite guests, prepare food, etc. By breaking this task into smaller sub-tasks such as buying decorations, inviting guests, and preparing food, you can manage each part of the event more effectively.\"\n",
      "    },\n",
      "    \"command\": {\n",
      "        \"name\": \"command name\",\n",
      "        \"args\": {\n",
      "            \"arg name\": \"value\"\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = chat(\"Can you give me an example?\")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T03:14:11.883029Z",
     "start_time": "2024-07-09T03:14:08.805918Z"
    }
   },
   "id": "56224058cd4f353a",
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你之前问的问题是关于\"Task Decomposition\"的概念。\n"
     ]
    }
   ],
   "source": [
    "response = chat(\"我刚才问了什么,hhhh?\")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T03:14:50.596266Z",
     "start_time": "2024-07-09T03:14:48.678355Z"
    }
   },
   "id": "be27c11fb81420aa",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<bound method ConversationBufferMemory.load_memory_variables of ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='What is Task Decomposition?'), AIMessage(content='Task Decomposition is a method used in artificial intelligence where a large task is broken down into smaller, more manageable parts called subtasks. This approach helps the AI system understand the task better and perform it more efficiently.'), HumanMessage(content='Can you give me an example?'), AIMessage(content='{\\n    \"thoughts\": {\\n        \"text\": \"The user wants an example of task decomposition.\",\\n        \"reasoning\": \"I will provide an example of how task decomposition works in a simple scenario.\",\\n        \"plan\": \"- Provide an example of task decomposition\\\\n- Explain how it breaks down the task into smaller parts\\\\n- Suggest that the user provides feedback on whether they understood the concept\",\\n        \"criticism\": \"It\\'s important to ensure the explanation is clear and understandable.\",\\n        \"speak\": \"Sure, here\\'s an example: Let\\'s break down the task of cooking a meal.\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}'), HumanMessage(content='我刚才问了什么?'), AIMessage(content='你之前问的问题是关于\"Task Decomposition\"的概念。')]), return_messages=True)>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T02:53:59.816774Z",
     "start_time": "2024-07-09T02:53:59.805268Z"
    }
   },
   "id": "745c6b4612222ebd",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[HumanMessage(content='What is Task Decomposition?'),\n AIMessage(content='Task Decomposition is a method used in artificial intelligence where a large task is broken down into smaller, more manageable parts called subtasks. This approach helps the AI system understand the task better and perform it more efficiently.'),\n HumanMessage(content='Can you give me an example?'),\n AIMessage(content='{\\n    \"thoughts\": {\\n        \"text\": \"The user wants an example of task decomposition.\",\\n        \"reasoning\": \"I will provide an example of how task decomposition works in a simple scenario.\",\\n        \"plan\": \"- Provide an example of task decomposition\\\\n- Explain how it breaks down the task into smaller parts\\\\n- Suggest that the user provides feedback on whether they understood the concept\",\\n        \"criticism\": \"It\\'s important to ensure the explanation is clear and understandable.\",\\n        \"speak\": \"Sure, here\\'s an example: Let\\'s break down the task of cooking a meal.\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}'),\n HumanMessage(content='我刚才问了什么?'),\n AIMessage(content='你之前问的问题是关于\"Task Decomposition\"的概念。')]"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.chat_memory.messages"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T03:03:55.706434Z",
     "start_time": "2024-07-09T03:03:55.698031Z"
    }
   },
   "id": "a95130af15c4656b",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T06:11:08.840153Z",
     "start_time": "2024-07-09T06:11:08.741759Z"
    }
   },
   "id": "9f544a9533e5df0a",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 方式2 RunnableWithMessageHistory"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27ddb2370752ec5a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "        RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"retriever_context\"])))\n",
    "        | prompt\n",
    "        | qw_llm_openai\n",
    "        | StrOutputParser()\n",
    ")\n",
    "\n",
    "retrieve_docs = (lambda x: x[\"input\"]) | retriever\n",
    "# retrieve_docs = lambda x: {\"retriever_context\": retriever.get_relevant_documents(x[\"input\"])}\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(retriever_context=retrieve_docs)\n",
    "    .assign(answer=rag_chain_from_docs)\n",
    ")\n",
    "\n",
    "\n",
    "def get_session_history():\n",
    "    return ChatMessageHistory()\n",
    "\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "def chat(input_text, session_id):\n",
    "    result = chain_with_history.invoke(\n",
    "        {\"input\": input_text},\n",
    "        config={\"configurable\": {\"session_id\": session_id}}\n",
    "    )\n",
    "    return result[\"answer\"]\n",
    "\n",
    "# 使用方法\n",
    "# response = chat(\"What is Task Decomposition?\", \"user_1\")\n",
    "# print(response)\n",
    "# 继续对话\n",
    "# response = chat(\"Can you give me an example?\", \"user_1\")\n",
    "# print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T03:02:28.235517Z",
     "start_time": "2024-07-09T03:02:28.224171Z"
    }
   },
   "id": "5d6bb9707ca81a6b",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_session_history() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[35], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mchat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mWhat is Task Decomposition?\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser_1\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(response)\n",
      "Cell \u001B[0;32mIn[34], line 60\u001B[0m, in \u001B[0;36mchat\u001B[0;34m(input_text, session_id)\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mchat\u001B[39m(input_text, session_id):\n\u001B[0;32m---> 60\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mchain_with_history\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     61\u001B[0m \u001B[43m        \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_text\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     62\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mconfigurable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msession_id\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43msession_id\u001B[49m\u001B[43m}\u001B[49m\u001B[43m}\u001B[49m\n\u001B[1;32m     63\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     64\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124manswer\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:4590\u001B[0m, in \u001B[0;36mRunnableBindingBase.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m   4582\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[1;32m   4583\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   4584\u001B[0m     \u001B[38;5;28minput\u001B[39m: Input,\n\u001B[1;32m   4585\u001B[0m     config: Optional[RunnableConfig] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   4586\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Optional[Any],\n\u001B[1;32m   4587\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Output:\n\u001B[1;32m   4588\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbound\u001B[38;5;241m.\u001B[39minvoke(\n\u001B[1;32m   4589\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m-> 4590\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_merge_configs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m   4591\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m{\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwargs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs},\n\u001B[1;32m   4592\u001B[0m     )\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/langchain_core/runnables/history.py:544\u001B[0m, in \u001B[0;36mRunnableWithMessageHistory._merge_configs\u001B[0;34m(self, *configs)\u001B[0m\n\u001B[1;32m    540\u001B[0m parameter_names \u001B[38;5;241m=\u001B[39m _get_parameter_names(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_session_history)\n\u001B[1;32m    542\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(expected_keys) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    543\u001B[0m     \u001B[38;5;66;03m# If arity = 1, then invoke function by positional arguments\u001B[39;00m\n\u001B[0;32m--> 544\u001B[0m     message_history \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_session_history\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfigurable\u001B[49m\u001B[43m[\u001B[49m\u001B[43mexpected_keys\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    545\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    546\u001B[0m     \u001B[38;5;66;03m# otherwise verify that names of keys patch and invoke by named arguments\u001B[39;00m\n\u001B[1;32m    547\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mset\u001B[39m(expected_keys) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mset\u001B[39m(parameter_names):\n",
      "\u001B[0;31mTypeError\u001B[0m: get_session_history() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "response = chat(\"What is Task Decomposition?\", \"user_1\")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T03:02:34.853862Z",
     "start_time": "2024-07-09T03:02:34.692379Z"
    }
   },
   "id": "96af98a76b7f67ff",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "牛背山你知道吗\n"
     ]
    }
   ],
   "source": [
    "# 假设 content 是一个包含 Unicode 编码的字符串\n",
    "content = b'\\u725b\\u80cc\\u5c71\\u4f60\\u77e5\\u9053\\u5417'\n",
    "\n",
    "# 使用 decode 方法将编码的字符串转换为正常的中文字符\n",
    "decoded_content = content.decode('unicode_escape')\n",
    "\n",
    "print(decoded_content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T03:57:40.971513Z",
     "start_time": "2024-07-09T03:57:40.966307Z"
    }
   },
   "id": "13f7d0fb8880a3b0",
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\u725b\\u80cc\\u5c71\\u4f60\\u77e5\\u9053\\u5417\n"
     ]
    }
   ],
   "source": [
    "# 假设 byte_content 是从外部获取的字节序列\n",
    "byte_content = b'\\xe7\\x88\\x86\\xe8\\x99\\x8e\\xe5\\xb1\\xb1\\xe4\\xbd\\xa0\\xe7\\x9f\\xa5\\xe9\\x81\\x93\\xe5\\x90\\x97'\n",
    "\n",
    "# 使用 decode 方法将字节序列解码为字符串\n",
    "content = content.decode('utf-8')\n",
    "\n",
    "print(content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-09T03:57:43.838185Z",
     "start_time": "2024-07-09T03:57:43.834515Z"
    }
   },
   "id": "7b47a32fd71a2f5c",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ff27f02e6ec67669"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
