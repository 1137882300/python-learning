{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GPTCache :一个致力于为存储 LLM 响应构建语义缓存的开源项目。"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install gptcache --quiet"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T08:04:18.361794Z",
     "start_time": "2024-07-08T08:04:15.547157Z"
    }
   },
   "id": "5e5a3c38e10c8293",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 基本配置\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "cf_llm_openai = ChatOpenAI(\n",
    "    openai_api_base=os.getenv('CF_API_BASE'),\n",
    "    openai_api_key=os.getenv('CF_API_TOKEN'),\n",
    "    model_name=\"@cf/meta/llama-3-8b-instruct\",\n",
    "    temperature=0,\n",
    "    streaming=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T08:06:52.961618Z",
     "start_time": "2024-07-08T08:06:52.595902Z"
    }
   },
   "id": "5fea22f348e2c14a",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "qw_llm_openai = ChatOpenAI(\n",
    "    openai_api_base=os.getenv('DASHSCOPE_API_BASE'),\n",
    "    openai_api_key=os.getenv('DASHSCOPE_API_KEY'),\n",
    "    model_name=\"qwen2-1.5b-instruct\",\n",
    "    temperature=0,\n",
    "    streaming=True,\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T08:12:08.255679Z",
     "start_time": "2024-07-08T08:12:08.223949Z"
    }
   },
   "id": "4009e2258ecfd31b",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain_community.embeddings.cloudflare_workersai import CloudflareWorkersAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "embedding = CloudflareWorkersAIEmbeddings(\n",
    "    account_id=os.getenv('CF_ACCOUNT_ID'),\n",
    "    api_token=os.getenv('CF_API_TOKEN'),\n",
    "    model_name=\"@cf/baai/bge-small-en-v1.5\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T08:15:09.748177Z",
     "start_time": "2024-07-08T08:15:09.733742Z"
    }
   },
   "id": "766ad7384893d65a",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "精确匹配"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c6e880f6d14faf4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import langchain\n",
    "from gptcache import Cache\n",
    "from gptcache.manager.factory import manager_factory\n",
    "from gptcache.processor.pre import get_prompt\n",
    "from gptcache.adapter.api import init_similar_cache\n",
    "from langchain.cache import GPTCache\n",
    "import hashlib\n",
    "\n",
    "\n",
    "def get_hashed_name(name):\n",
    "    return hashlib.sha256(name.encode()).hexdigest()\n",
    "\n",
    "\n",
    "def init_gptcache(cache_obj: Cache, llm: str):\n",
    "    hashed_llm = get_hashed_name(llm)\n",
    "    cache_obj.init(\n",
    "        pre_embedding_func=get_prompt,\n",
    "        data_manager=manager_factory(manager=\"map\", data_dir=f\"map_cache_{hashed_llm}\"),\n",
    "    )\n",
    "\n",
    "\n",
    "langchain.llm_cache = GPTCache(init_gptcache)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T08:06:25.612929Z",
     "start_time": "2024-07-08T08:06:25.150985Z"
    }
   },
   "id": "72945df509b93688",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "question = \"What is cache eviction policy?\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T08:07:01.520853Z",
     "start_time": "2024-07-08T08:07:01.494735Z"
    }
   },
   "id": "24c38488a5f97b08",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 697 ms, sys: 45.2 ms, total: 743 ms\n",
      "Wall time: 32.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "AIMessage(content='Cache eviction policy, also known as cache replacement policy, is a set of rules that defines how a cache (a small, fast memory location) handles the situation where it needs to remove an item to make room for a new one. This is known as cache eviction. The goal of the policy is to ensure that the cache remains as effective as possible, by minimizing the number of times items need to be retrieved from slower, larger storage devices (such as hard drives).\\n\\nThere are several common cache eviction policies:\\n\\n1. **FIFO (First-In-First-Out)**: The policy removes the item that has been in the cache the longest (FIFO) to make room for the new item. This is the simplest policy to implement.\\n\\n2. **LRU (Least Recently Used)**: The policy removes the item that has not been accessed for the longest time (LRU) to make room for the new item. This policy is more effective than FIFO as it favors the items that are most likely to be reused.\\n\\n3. **LFU (Least Frequently Used)**: The policy removes the item that has been accessed the least in the past (LFU) to make room for the new item. This policy is useful in scenarios where some items are more popular than others.\\n\\n4. **OPT (Optimal)**: The policy removes the item that will not be needed in the future to make room for the new item. This policy is theoretical and is not practical to implement.\\n\\n5. **LIRS (Least Recently Used Insertion Sequence)**: This policy is a combination of LRU and FIFO, where the item that has been inserted last and has not been accessed for the longest time is removed.\\n\\nThe choice of cache eviction policy depends on the specific requirements of the system, such as the type of data stored, the memory size of the cache, and the access patterns of the data.\\n\\nIn conclusion, cache eviction policy is a crucial component of any caching system, as it determines how the cache handles the removal of items to make room for new ones. Effective cache eviction policies can significantly improve the performance and efficiency of systems that rely on caching.', id='run-82b4776e-54db-4cc9-a6bf-c7427b6a9038-0')"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cf_llm_openai.invoke(question)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T08:07:34.019103Z",
     "start_time": "2024-07-08T08:07:01.955823Z"
    }
   },
   "id": "3ce6fa2ad4ab2d63",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.57 ms, sys: 2.24 ms, total: 3.81 ms\n",
      "Wall time: 3.6 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": "AIMessage(content='Cache eviction policy, also known as cache replacement policy, is a set of rules that defines how a cache (a small, fast memory location) handles the situation where it needs to remove an item to make room for a new one. This is known as cache eviction. The goal of the policy is to ensure that the cache remains as effective as possible, by minimizing the number of times items need to be retrieved from slower, larger storage devices (such as hard drives).\\n\\nThere are several common cache eviction policies:\\n\\n1. **FIFO (First-In-First-Out)**: The policy removes the item that has been in the cache the longest (FIFO) to make room for the new item. This is the simplest policy to implement.\\n\\n2. **LRU (Least Recently Used)**: The policy removes the item that has not been accessed for the longest time (LRU) to make room for the new item. This policy is more effective than FIFO as it favors the items that are most likely to be reused.\\n\\n3. **LFU (Least Frequently Used)**: The policy removes the item that has been accessed the least in the past (LFU) to make room for the new item. This policy is useful in scenarios where some items are more popular than others.\\n\\n4. **OPT (Optimal)**: The policy removes the item that will not be needed in the future to make room for the new item. This policy is theoretical and is not practical to implement.\\n\\n5. **LIRS (Least Recently Used Insertion Sequence)**: This policy is a combination of LRU and FIFO, where the item that has been inserted last and has not been accessed for the longest time is removed.\\n\\nThe choice of cache eviction policy depends on the specific requirements of the system, such as the type of data stored, the memory size of the cache, and the access patterns of the data.\\n\\nIn conclusion, cache eviction policy is a crucial component of any caching system, as it determines how the cache handles the removal of items to make room for new ones. Effective cache eviction policies can significantly improve the performance and efficiency of systems that rely on caching.', id='run-82b4776e-54db-4cc9-a6bf-c7427b6a9038-0')"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cf_llm_openai.invoke(question)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T08:10:06.188912Z",
     "start_time": "2024-07-08T08:10:06.185616Z"
    }
   },
   "id": "fa1573716444cf4b",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 425 ms, sys: 35.9 ms, total: 460 ms\n",
      "Wall time: 33.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "AIMessage(content='Cache eviction policy, also known as cache replacement policy, is a strategy used by a cache (a small, fast memory that stores frequently-used data) to decide which items to remove (evict) when the cache is full and new data needs to be stored. The goal of cache eviction policy is to minimize the number of cache misses (when data is not found in the cache) and maintain optimal performance.\\n\\nThere are several cache eviction policies, each with its own trade-offs and characteristics. Here are some common ones:\\n\\n1. **LRU (Least Recently Used)**: Removes the item that has not been accessed for the longest time. This policy assumes that frequently used items will be accessed again soon.\\n2. **FIFO (First-In-First-Out)**: Removes the item that was added to the cache first. This policy is simple but can be less efficient than LRU.\\n3. **LRU-K (Least Recently Used with a cost associated with each item)**: Assigns a cost to each item based on its size or access frequency. Removes the item with the highest cost.\\n4. **Random Replacement**: Chooses a random item to evict when the cache is full. This policy is simple but can lead to poor performance.\\n5. **FIFO-LRU**: Combines FIFO and LRU policies. It first tries to evict an item from the front of the queue (FIFO) and if the cache is still full, it uses LRU to evict the least recently used item.\\n6. **LFU (Least Frequently Used)**: Removes the item that was accessed the least frequently. This policy assumes that infrequently used items will not be accessed again soon.\\n7. **Gaussian Distribution**: Uses statistical analysis to determine the likelihood of an item being accessed in the future and evicts the item with the lowest probability of being accessed.\\n8. **CacheSim**: A more complex policy that uses a simulated annealing algorithm to optimize cache eviction.\\n\\nWhen selecting a cache eviction policy, consider factors such as:\\n\\n* Access patterns: Frequent access patterns may benefit from LRU or LFU policies.\\n* Cache size: Larger caches can use more complex policies like LRU-K or Gaussian Distribution.\\n* System requirements: Real-time systems may require a more predictable cache eviction policy, like LRU or FIFO.\\n\\nBy choosing the right cache eviction policy, you can optimize the performance of your system and minimize cache misses.', id='run-b8e93b39-28b8-427c-8c7e-8f8e1b2f13f3-0')"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cf_llm_openai.invoke('What is cache eviction   policy?')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T08:11:02.922195Z",
     "start_time": "2024-07-08T08:10:29.325388Z"
    }
   },
   "id": "c1bcc428e2c132ea",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similar Match\n",
    "相似匹配"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "defa990345a8d49f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from gptcache import Cache\n",
    "from gptcache.adapter.api import init_similar_cache\n",
    "from langchain.cache import GPTCache\n",
    "import hashlib\n",
    "\n",
    "\n",
    "def get_hashed_name(name):\n",
    "    return hashlib.sha256(name.encode()).hexdigest()\n",
    "\n",
    "\n",
    "# cf embedding 当前不支持\n",
    "def init_gptcache(cache_obj: Cache, llm: str):\n",
    "    hashed_llm = get_hashed_name(llm)\n",
    "    init_similar_cache(cache_obj=cache_obj, data_dir=f\"similar_cache_{hashed_llm}\", embedding=embedding)\n",
    "\n",
    "\n",
    "langchain.llm_cache = GPTCache(init_gptcache)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T08:15:13.207376Z",
     "start_time": "2024-07-08T08:15:13.202824Z"
    }
   },
   "id": "3b533c26257dd91b",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CloudflareWorkersAIEmbeddings' object has no attribute 'dimension'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[0;32m<timed eval>:1\u001B[0m\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:248\u001B[0m, in \u001B[0;36mBaseChatModel.invoke\u001B[0;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[1;32m    237\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[1;32m    238\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    239\u001B[0m     \u001B[38;5;28minput\u001B[39m: LanguageModelInput,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    243\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    244\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseMessage:\n\u001B[1;32m    245\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[1;32m    246\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[1;32m    247\u001B[0m         ChatGeneration,\n\u001B[0;32m--> 248\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    249\u001B[0m \u001B[43m            \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    250\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    251\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcallbacks\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    252\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtags\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmetadata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrun_name\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrun_id\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m    258\u001B[0m     )\u001B[38;5;241m.\u001B[39mmessage\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:677\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[1;32m    670\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    671\u001B[0m     prompts: List[PromptValue],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    674\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    675\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    676\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[0;32m--> 677\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_messages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:534\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    532\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n\u001B[1;32m    533\u001B[0m             run_managers[i]\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[0;32m--> 534\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    535\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    536\u001B[0m     LLMResult(generations\u001B[38;5;241m=\u001B[39m[res\u001B[38;5;241m.\u001B[39mgenerations], llm_output\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39mllm_output)  \u001B[38;5;66;03m# type: ignore[list-item]\u001B[39;00m\n\u001B[1;32m    537\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[1;32m    538\u001B[0m ]\n\u001B[1;32m    539\u001B[0m llm_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_combine_llm_outputs([res\u001B[38;5;241m.\u001B[39mllm_output \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results])\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:524\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    521\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(messages):\n\u001B[1;32m    522\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    523\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m--> 524\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_with_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    525\u001B[0m \u001B[43m                \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    526\u001B[0m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    527\u001B[0m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    528\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    529\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    530\u001B[0m         )\n\u001B[1;32m    531\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    532\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:710\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    708\u001B[0m llm_string \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_llm_string(stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    709\u001B[0m prompt \u001B[38;5;241m=\u001B[39m dumps(messages)\n\u001B[0;32m--> 710\u001B[0m cache_val \u001B[38;5;241m=\u001B[39m \u001B[43mllm_cache\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlookup\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mllm_string\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    711\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(cache_val, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m    712\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ChatResult(generations\u001B[38;5;241m=\u001B[39mcache_val)\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/langchain_community/cache.py:818\u001B[0m, in \u001B[0;36mGPTCache.lookup\u001B[0;34m(self, prompt, llm_string)\u001B[0m\n\u001B[1;32m    812\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Look up the cache data.\u001B[39;00m\n\u001B[1;32m    813\u001B[0m \u001B[38;5;124;03mFirst, retrieve the corresponding cache object using the `llm_string` parameter,\u001B[39;00m\n\u001B[1;32m    814\u001B[0m \u001B[38;5;124;03mand then retrieve the data from the cache based on the `prompt`.\u001B[39;00m\n\u001B[1;32m    815\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    816\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgptcache\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01madapter\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get\n\u001B[0;32m--> 818\u001B[0m _gptcache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_gptcache\u001B[49m\u001B[43m(\u001B[49m\u001B[43mllm_string\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    820\u001B[0m res \u001B[38;5;241m=\u001B[39m get(prompt, cache_obj\u001B[38;5;241m=\u001B[39m_gptcache)\n\u001B[1;32m    821\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _loads_generations(res) \u001B[38;5;28;01mif\u001B[39;00m res \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/langchain_community/cache.py:808\u001B[0m, in \u001B[0;36mGPTCache._get_gptcache\u001B[0;34m(self, llm_string)\u001B[0m\n\u001B[1;32m    806\u001B[0m _gptcache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgptcache_dict\u001B[38;5;241m.\u001B[39mget(llm_string, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    807\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _gptcache:\n\u001B[0;32m--> 808\u001B[0m     _gptcache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_new_gptcache\u001B[49m\u001B[43m(\u001B[49m\u001B[43mllm_string\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    809\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _gptcache\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/langchain_community/cache.py:790\u001B[0m, in \u001B[0;36mGPTCache._new_gptcache\u001B[0;34m(self, llm_string)\u001B[0m\n\u001B[1;32m    788\u001B[0m sig \u001B[38;5;241m=\u001B[39m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minit_gptcache_func)\n\u001B[1;32m    789\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(sig\u001B[38;5;241m.\u001B[39mparameters) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[0;32m--> 790\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minit_gptcache_func\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_gptcache\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mllm_string\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m    791\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    792\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minit_gptcache_func(_gptcache)  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[14], line 13\u001B[0m, in \u001B[0;36minit_gptcache\u001B[0;34m(cache_obj, llm)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minit_gptcache\u001B[39m(cache_obj: Cache, llm: \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m     12\u001B[0m     hashed_llm \u001B[38;5;241m=\u001B[39m get_hashed_name(llm)\n\u001B[0;32m---> 13\u001B[0m     \u001B[43minit_similar_cache\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcache_obj\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_obj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msimilar_cache_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mhashed_llm\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43membedding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/venv/lib/python3.10/site-packages/gptcache/adapter/api.py:179\u001B[0m, in \u001B[0;36minit_similar_cache\u001B[0;34m(data_dir, cache_obj, pre_func, embedding, data_manager, evaluation, post_func, config)\u001B[0m\n\u001B[1;32m    174\u001B[0m     embedding \u001B[38;5;241m=\u001B[39m Onnx()\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data_manager:\n\u001B[1;32m    176\u001B[0m     data_manager \u001B[38;5;241m=\u001B[39m manager_factory(\n\u001B[1;32m    177\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msqlite,faiss\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    178\u001B[0m         data_dir\u001B[38;5;241m=\u001B[39mdata_dir,\n\u001B[0;32m--> 179\u001B[0m         vector_params\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdimension\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[43membedding\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdimension\u001B[49m},\n\u001B[1;32m    180\u001B[0m     )\n\u001B[1;32m    181\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m evaluation:\n\u001B[1;32m    182\u001B[0m     evaluation \u001B[38;5;241m=\u001B[39m SearchDistanceEvaluation()\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'CloudflareWorkersAIEmbeddings' object has no attribute 'dimension'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "qw_llm_openai.invoke(question)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-08T08:15:14.282949Z",
     "start_time": "2024-07-08T08:15:14.185155Z"
    }
   },
   "id": "8bcd3bc56beb4202",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "456fb2f8fb8c214d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "qw_llm_openai.invoke(\"What is cache eviction   policy?\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c050c66e859d9b0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "qw_llm_openai.invoke(\"Give me a peach\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91f3fd2d0571322"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "qw_llm_openai.invoke(\"Give me 2 peaches\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea7200cd0a22ea82"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
