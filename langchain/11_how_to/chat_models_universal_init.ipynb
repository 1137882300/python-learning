{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "🪄 在一行代码中初始化任何模型\n",
    "LangChain 有很多聊天模型集成，可能很难记住如何导入每个集成。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9705406023c185b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Basic usage\n",
    "# Returns a langchain_openai.ChatOpenAI instance.\n",
    "gpt_4o = init_chat_model(\"gpt-4o\", model_provider=\"openai\", temperature=0)\n",
    "# Returns a langchain_anthropic.ChatAnthropic instance.\n",
    "claude_opus = init_chat_model(\n",
    "    \"claude-3-opus-20240229\", model_provider=\"anthropic\", temperature=0\n",
    ")\n",
    "# Returns a langchain_google_vertexai.ChatVertexAI instance.\n",
    "gemini_15 = init_chat_model(\n",
    "    \"gemini-1.5-pro\", model_provider=\"google_vertexai\", temperature=0\n",
    ")\n",
    "\n",
    "# Since all model integrations implement the ChatModel interface, you can use them in the same way.\n",
    "print(\"GPT-4o: \" + gpt_4o.invoke(\"what's your name\").content + \"\\n\")\n",
    "print(\"Claude Opus: \" + claude_opus.invoke(\"what's your name\").content + \"\\n\")\n",
    "print(\"Gemini 1.5: \" + gemini_15.invoke(\"what's your name\").content + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 推理模型提供程序\n",
    "\n",
    "gpt_4o = init_chat_model(\"gpt-4o\", temperature=0)\n",
    "claude_opus = init_chat_model(\"claude-3-opus-20240229\", temperature=0)\n",
    "gemini_15 = init_chat_model(\"gemini-1.5-pro\", temperature=0)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b1846cc02aeaa8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 创建可配置模型\n",
    "# 您还可以通过指定 configurable_fields 来创建运行时可配置的模型。如果您未指定 model 值，则默认情况下 “model” 和 “model_provider” 是可配置的。\n",
    "\n",
    "configurable_model = init_chat_model(temperature=0)\n",
    "\n",
    "configurable_model.invoke(\n",
    "    \"what's your name\", config={\"configurable\": {\"model\": \"gpt-4o\"}}\n",
    ")\n",
    "\n",
    "configurable_model.invoke(\n",
    "    \"what's your name\", config={\"configurable\": {\"model\": \"claude-3-5-sonnet-20240620\"}}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2012beea4f0d3617"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 具有默认值的可配置模型\n",
    "# 我们可以创建一个具有默认模型值的可配置模型，指定哪些参数是可配置的，并为可配置的参数添加前缀：\n",
    "\n",
    "first_llm = init_chat_model(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    configurable_fields=(\"model\", \"model_provider\", \"temperature\", \"max_tokens\"),\n",
    "    config_prefix=\"first\",  # useful when you have a chain with multiple models\n",
    ")\n",
    "\n",
    "first_llm.invoke(\"what's your name\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e2c11182c865632"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "first_llm.invoke(\n",
    "    \"what's your name\",\n",
    "    config={\n",
    "        \"configurable\": {\n",
    "            \"first_model\": \"claude-3-5-sonnet-20240620\",\n",
    "            \"first_temperature\": 0.5,\n",
    "            \"first_max_tokens\": 100,\n",
    "        }\n",
    "    },\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "114ef50b9d179fc5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 以声明方式使用可配置模型\n",
    "# 我们可以在可配置模型上调用 bind_tools、with_structured_output、with_configurable 等声明性操作，\n",
    "# 并以与常规实例化聊天模型对象相同的方式链接可配置模型。\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class GetWeather(BaseModel):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "\n",
    "    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
    "\n",
    "\n",
    "class GetPopulation(BaseModel):\n",
    "    \"\"\"Get the current population in a given location\"\"\"\n",
    "\n",
    "    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
    "\n",
    "\n",
    "llm = init_chat_model(temperature=0)\n",
    "llm_with_tools = llm.bind_tools([GetWeather, GetPopulation])\n",
    "\n",
    "llm_with_tools.invoke(\n",
    "    \"what's bigger in 2024 LA or NYC\", config={\"configurable\": {\"model\": \"gpt-4o\"}}\n",
    ").tool_calls"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e827e5b079cd3a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "llm_with_tools.invoke(\n",
    "    \"what's bigger in 2024 LA or NYC\",\n",
    "    config={\"configurable\": {\"model\": \"claude-3-5-sonnet-20240620\"}},\n",
    ").tool_calls"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37d5d43a06e2e5ea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
