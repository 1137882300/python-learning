{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet  langchainhub   langchain-chroma bs4"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:03:47.443899Z",
     "start_time": "2024-07-04T10:03:44.203726Z"
    }
   },
   "id": "c7d6b24ebd9eaf2b",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:08:29.413184Z",
     "start_time": "2024-07-04T10:08:29.409967Z"
    }
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 无聊天记录的链"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "214cf378f2a42e0b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 基本配置\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.embeddings.cloudflare_workersai import CloudflareWorkersAIEmbeddings\n",
    "from supabase.client import Client, create_client\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "qw_llm_openai = ChatOpenAI(\n",
    "    openai_api_base=os.getenv('DASHSCOPE_API_BASE'),\n",
    "    openai_api_key=os.getenv('DASHSCOPE_API_KEY'),\n",
    "    model_name=\"qwen2-1.5b-instruct\",\n",
    "    temperature=0.7,\n",
    "    streaming=True,\n",
    ")\n",
    "embeddings = CloudflareWorkersAIEmbeddings(\n",
    "    account_id=os.getenv('CF_ACCOUNT_ID'),\n",
    "    api_token=os.getenv('CF_API_TOKEN'),\n",
    "    model_name=\"@cf/baai/bge-small-en-v1.5\",\n",
    ")\n",
    "\n",
    "# supabase_url = os.environ.get(\"SUPABASE_URL\")\n",
    "# supabase_key = os.environ.get(\"SUPABASE_SERVICE_KEY\")\n",
    "# \n",
    "# supabase: Client = create_client(supabase_url, supabase_key)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:08:41.727462Z",
     "start_time": "2024-07-04T10:08:41.649313Z"
    }
   },
   "id": "acad47c9f0d9bc16",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 18:08:54,489:INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n"
     ]
    }
   ],
   "source": [
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "persist_directory = '/Users/pangmengting/Documents/workspace/python-learning/langchain/history/chroma-data'\n",
    "collection_name = 'history_index'\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings,\n",
    "                                    collection_name=collection_name,\n",
    "                                    persist_directory=persist_directory)\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | qw_llm_openai\n",
    "        | StrOutputParser()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:09:02.406143Z",
     "start_time": "2024-07-04T10:08:44.268723Z"
    }
   },
   "id": "1aba8cd4386bd9b",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 18:09:23,037:INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": "'Task Decomposition is a method used to break down complex tasks into smaller, simpler steps. This technique involves chaining of thoughts and generating multiple thoughts per step, creating a tree structure. It can be done manually through simple prompts or automatically using task-specific instructions or human inputs.'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:09:23.466481Z",
     "start_time": "2024-07-04T10:09:19.465225Z"
    }
   },
   "id": "d02a2d9e6b7c0848",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 将问题置于背景中\n",
    "\n",
    "# 首先，我们需要定义一个子链，它接受历史消息和最新的用户问题，如果它引用了历史信息中的任何信息，则重新定义问题。/\n",
    "\n",
    "# 我们将使用一个包含名为“chat_history”的 MessagesPlaceholder 变量的提示符。这允许我们使用“chat_history”输入键向提示符传递消息列表，\n",
    "# 这些消息将插入在系统消息之后，包含最新问题的人工消息之前。\n",
    "\n",
    "# 请注意，我们在此步骤中使用了一个辅助函数create_history_aware_retriever，它管理 chat_history 为空的情况，\n",
    "# 否则按顺序应用 prompt | llm | StrOutputParser() | retriever 。\n",
    "\n",
    "# create_history_aware_retriever 构造了一个链，接受键 input 和 chat_history 作为输入，并具有与检索器相同的输出模式。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "553acbf1327ec533"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "# from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# =====\n",
    "# 中文: 给定一个聊天历史记录和最新的用户问题，该问题可能参考了聊天历史记录中的上下文，制定一个独立的问题，\n",
    "# 这个问题无需聊天历史记录就能理解。不要回答这个问题，如果需要，只需重新表述它，否则就按原样返回。\n",
    "# =====\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    qw_llm_openai, retriever, contextualize_q_prompt\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:16:28.854807Z",
     "start_time": "2024-07-04T10:16:28.845826Z"
    }
   },
   "id": "d2740084d0870258",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 带聊天记录的链\n",
    "\n",
    "# 现在我们可以建立完整的QA链。\n",
    "\n",
    "# 在这里，我们使用create_stuff_documents_chain生成一个 question_answer_chain ，\n",
    "# 输入键为 context 、 chat_history 和 input --它接受检索到的上下文以及会话历史和查询，以生成一个答案。\n",
    "\n",
    "# 我们使用create_retrieval_chain构建最终的 rag_chain 。这个链按顺序应用 history_aware_retriever 和 question_answer_chain ，\n",
    "# 为了方便起见，保留了中间输出，\n",
    "# 如检索到的上下文。它有输入键 input 和 chat_history ，并在其输出中包括 input 、 chat_history 、 context 和 answer 。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca6d0a5166ea6fb9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "# from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "# =====\n",
    "# -  🔤 中文: 你是回答问题任务的助手。使用以下检索到的上下文片段来回答问题。如果你不知道答案，就说你不知道。最多使用三句话并保持答案简洁。\n",
    "# =====\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(qw_llm_openai, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:22:12.463430Z",
     "start_time": "2024-07-04T10:22:12.389805Z"
    }
   },
   "id": "d5a43820fb67f501",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 18:22:51,014:INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg_1[\"answer\"]])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:22:51.610113Z",
     "start_time": "2024-07-04T10:22:47.696996Z"
    }
   },
   "id": "8968e19cc02f27a1",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'input': 'What is Task Decomposition?',\n 'chat_history': [HumanMessage(content='What is Task Decomposition?'),\n  'Task Decomposition is a method used to break down complex tasks into smaller, more manageable ones. It involves breaking down a problem into multiple steps or subtasks and then analyzing the logic behind each step. This allows the agent to focus on specific aspects of the task while also considering the overall goal. It is commonly used in artificial intelligence and machine learning applications to help agents make sense of large amounts of data and perform tasks more efficiently.'],\n 'context': [Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n  Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n  Document(page_content='The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n  Document(page_content='Fig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})],\n 'answer': 'Task Decomposition is a method used to break down complex tasks into smaller, more manageable ones. It involves breaking down a problem into multiple steps or subtasks and then analyzing the logic behind each step. This allows the agent to focus on specific aspects of the task while also considering the overall goal. It is commonly used in artificial intelligence and machine learning applications to help agents make sense of large amounts of data and perform tasks more efficiently.'}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg_1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:23:00.045921Z",
     "start_time": "2024-07-04T10:23:00.040431Z"
    }
   },
   "id": "a98a2089ae8020e5",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 18:29:50,023:INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-04 18:29:52,439:INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are several ways to do task decomposition:\n",
      "\n",
      "  1. Using LLMs with simple prompts such as \"Steps for XYZ\" and \"What are the subgoals for achieving XYZ.\"\n",
      "  2. Using task-specific instructions like \"Write a story outline\" for writing a novel or \"Create a list of items needed for cooking dinner\" for meal preparation.\n",
      "  3. Human inputs to guide the decomposition process.\n",
      "\n",
      "Overall, task decomposition helps agents break down complex tasks into smaller, more manageable parts, allowing them to focus on specific aspects of the task while still considering the overall goal.\n"
     ]
    }
   ],
   "source": [
    "second_question = \"What are common ways of doing it?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:29:53.424326Z",
     "start_time": "2024-07-04T10:29:49.670006Z"
    }
   },
   "id": "fb8f4e645b7dcf3d",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[HumanMessage(content='What is Task Decomposition?'),\n 'Task Decomposition is a method used to break down complex tasks into smaller, more manageable ones. It involves breaking down a problem into multiple steps or subtasks and then analyzing the logic behind each step. This allows the agent to focus on specific aspects of the task while also considering the overall goal. It is commonly used in artificial intelligence and machine learning applications to help agents make sense of large amounts of data and perform tasks more efficiently.']"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:30:00.465513Z",
     "start_time": "2024-07-04T10:30:00.454698Z"
    }
   },
   "id": "f2d0547ee60f7715",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "chat_history.extend([HumanMessage(content=second_question), ai_msg_2[\"answer\"]])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:30:32.628627Z",
     "start_time": "2024-07-04T10:30:32.621773Z"
    }
   },
   "id": "11aa3f92ddf14208",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[HumanMessage(content='What is Task Decomposition?'),\n 'Task Decomposition is a method used to break down complex tasks into smaller, more manageable ones. It involves breaking down a problem into multiple steps or subtasks and then analyzing the logic behind each step. This allows the agent to focus on specific aspects of the task while also considering the overall goal. It is commonly used in artificial intelligence and machine learning applications to help agents make sense of large amounts of data and perform tasks more efficiently.',\n HumanMessage(content='What are common ways of doing it?'),\n 'There are several ways to do task decomposition:\\n\\n  1. Using LLMs with simple prompts such as \"Steps for XYZ\" and \"What are the subgoals for achieving XYZ.\"\\n  2. Using task-specific instructions like \"Write a story outline\" for writing a novel or \"Create a list of items needed for cooking dinner\" for meal preparation.\\n  3. Human inputs to guide the decomposition process.\\n\\nOverall, task decomposition helps agents break down complex tasks into smaller, more manageable parts, allowing them to focus on specific aspects of the task while still considering the overall goal.']"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:30:36.464942Z",
     "start_time": "2024-07-04T10:30:36.459384Z"
    }
   },
   "id": "297a0d24544e5ada",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 18:34:28,612:INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-04 18:34:32,043:INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG是一种基于规则的生成模型，它通过使用规则来生成文本。它的名字是Random Access Grammar，其中Access代表了可以访问的资源（如单词表、词典等），Grammar则表示生成规则。\n",
      "\n",
      "RAG模型通常用于自动问答系统中，因为它可以根据用户的问题生成答案。它可以将问题分解成一系列步骤，并在每个步骤中选择一个合适的输入或输出。这使得机器能够理解和回答复杂的问题，而不需要人类的帮助。\n"
     ]
    }
   ],
   "source": [
    "second_question = \"什么是rag?\"\n",
    "ai_msg_3 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_3[\"answer\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:34:33.158512Z",
     "start_time": "2024-07-04T10:34:28.247082Z"
    }
   },
   "id": "91b5470e2ae8a047",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'input': '什么是rag?',\n 'chat_history': [HumanMessage(content='What is Task Decomposition?'),\n  'Task Decomposition is a method used to break down complex tasks into smaller, more manageable ones. It involves breaking down a problem into multiple steps or subtasks and then analyzing the logic behind each step. This allows the agent to focus on specific aspects of the task while also considering the overall goal. It is commonly used in artificial intelligence and machine learning applications to help agents make sense of large amounts of data and perform tasks more efficiently.',\n  HumanMessage(content='What are common ways of doing it?'),\n  'There are several ways to do task decomposition:\\n\\n  1. Using LLMs with simple prompts such as \"Steps for XYZ\" and \"What are the subgoals for achieving XYZ.\"\\n  2. Using task-specific instructions like \"Write a story outline\" for writing a novel or \"Create a list of items needed for cooking dinner\" for meal preparation.\\n  3. Human inputs to guide the decomposition process.\\n\\nOverall, task decomposition helps agents break down complex tasks into smaller, more manageable parts, allowing them to focus on specific aspects of the task while still considering the overall goal.'],\n 'context': [Document(page_content='API-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n  Document(page_content='Case Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n  Document(page_content='Conversatin samples:\\n[\\n  {\\n    \"role\": \"system\",', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n  Document(page_content='The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})],\n 'answer': 'RAG是一种基于规则的生成模型，它通过使用规则来生成文本。它的名字是Random Access Grammar，其中Access代表了可以访问的资源（如单词表、词典等），Grammar则表示生成规则。\\n\\nRAG模型通常用于自动问答系统中，因为它可以根据用户的问题生成答案。它可以将问题分解成一系列步骤，并在每个步骤中选择一个合适的输入或输出。这使得机器能够理解和回答复杂的问题，而不需要人类的帮助。'}"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg_3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:34:45.875374Z",
     "start_time": "2024-07-04T10:34:45.866203Z"
    }
   },
   "id": "9b8befd2346ba913",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 18:36:02,432:INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG (Randomly Generated Assistant) 是一种人工智能技术，它结合了生成式模型和有监督学习。RAG 使用随机文本作为输入来训练一个基于上下文的机器翻译系统，并使用已有的语料库进行微调。\n",
      "\n",
      "RAG 的核心思想是将生成器（Generator）与解释器（Inverter）组合在一起，使得在生成文本时可以同时考虑生成内容的质量和语言结构的正确性。这种设计方式允许 RAG 在处理自然语言任务时能够自动生成高质量的内容，而无需显式地指定每一步的逻辑或规则。\n",
      "\n",
      "RAG 模型通常由两部分组成：生成器和解释器。生成器负责根据给定的初始文本生成后续的文本；解释器则用于评估生成文本的质量并提供反馈。这种设计模式使得 RAG 能够有效地利用现有的语言资源，如大量的训练数据、预训练模型等，从而快速提高其性能。\n"
     ]
    }
   ],
   "source": [
    "second_question = \"什么是rag?\"\n",
    "ai_msg_4 = rag_chain.invoke({\"input\": second_question, \"chat_history\": []})\n",
    "\n",
    "print(ai_msg_4[\"answer\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:36:04.479257Z",
     "start_time": "2024-07-04T10:35:59.796227Z"
    }
   },
   "id": "d252cb528604f79d",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'input': '什么是rag?',\n 'chat_history': [],\n 'context': [Document(page_content='Conversatin samples:\\n[\\n  {\\n    \"role\": \"system\",', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n  Document(page_content='Here are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n  Document(page_content='You should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n  Document(page_content='The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})],\n 'answer': 'RAG (Randomly Generated Assistant) 是一种人工智能技术，它结合了生成式模型和有监督学习。RAG 使用随机文本作为输入来训练一个基于上下文的机器翻译系统，并使用已有的语料库进行微调。\\n\\nRAG 的核心思想是将生成器（Generator）与解释器（Inverter）组合在一起，使得在生成文本时可以同时考虑生成内容的质量和语言结构的正确性。这种设计方式允许 RAG 在处理自然语言任务时能够自动生成高质量的内容，而无需显式地指定每一步的逻辑或规则。\\n\\nRAG 模型通常由两部分组成：生成器和解释器。生成器负责根据给定的初始文本生成后续的文本；解释器则用于评估生成文本的质量并提供反馈。这种设计模式使得 RAG 能够有效地利用现有的语言资源，如大量的训练数据、预训练模型等，从而快速提高其性能。'}"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg_4"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:36:11.682227Z",
     "start_time": "2024-07-04T10:36:11.671170Z"
    }
   },
   "id": "53756fc3fed7716c",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    " # 回返来源\n",
    "\n",
    "# 通常在问答应用程序中，向用户显示用于生成答案的来源是很重要的。\n",
    "# LangChain内置的 create_retrieval_chain 会将检索到的源文档传播到 \"context\" 键中的输出："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d92ecc51463dfb"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Conversatin samples:\\n[\\n  {\\n    \"role\": \"system\",' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "\n",
      "page_content='Here are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "\n",
      "page_content='You should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "\n",
      "page_content='The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.' metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n"
     ]
    }
   ],
   "source": [
    "for document in ai_msg_4[\"context\"]:\n",
    "    print(document)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:37:49.244206Z",
     "start_time": "2024-07-04T10:37:49.238333Z"
    }
   },
   "id": "671706217f6e6a94",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 在这里，我们已经讨论了如何添加应用程序逻辑来合并历史输出，但我们仍然手动更新聊天历史并将其插入到每个输入中。\n",
    "# 在一个真实的Q&A应用程序中，我们需要一些方法来保存聊天记录，以及一些自动插入和更新聊天记录的方法。\n",
    "\n",
    "# 为此，我们可以用途：\n",
    "\n",
    "# BaseChatMessageHistory：存储聊天记录。\n",
    "# RunnableWithMessageHistory：LCEL链和 BaseChatMessageHistory 的包装器，用于处理将聊天历史注入输入并在每次调用后更新它。\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a994a87253760509"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 实现第二个选项的一个简单示例，其中聊天历史记录存储在一个简单的dict中。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c8ebc2562856b38"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import bs4\n",
    "# from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "### Construct retriever ###\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "persist_directory = '/Users/pangmengting/Documents/workspace/python-learning/langchain/history/chroma-data'\n",
    "collection_name = 'history2_index'\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings,\n",
    "                                    collection_name=collection_name,\n",
    "                                    persist_directory=persist_directory)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "### Contextualize question ###\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    qw_llm_openai, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "### Answer question ###\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(qw_llm_openai, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "### Statefully manage chat history ###\n",
    "# 有状态地管理聊天记录\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# RunnableWithMessageHistory 允许我们将消息历史添加到某些类型的链中。它包装另一个Runnable并管理它的聊天消息历史记录。\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:47:56.617924Z",
     "start_time": "2024-07-04T10:47:48.572882Z"
    }
   },
   "id": "9eb7dfd85cf02b45",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 18:48:18,576:INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": "'Task Decomposition is a method used to break down complex tasks into smaller, more manageable ones. It involves breaking down a task into multiple steps or subtasks and then analyzing the logic behind each step. This allows the agent to focus on specific aspects of the task while also ensuring that it completes all necessary steps.'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is Task Decomposition?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:48:19.002492Z",
     "start_time": "2024-07-04T10:48:15.597848Z"
    }
   },
   "id": "85a7f18a1dda6f2b",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'abc123': InMemoryChatMessageHistory(messages=[HumanMessage(content='What is Task Decomposition?'), AIMessage(content='Task Decomposition is a method used to break down complex tasks into smaller, more manageable ones. It involves breaking down a task into multiple steps or subtasks and then analyzing the logic behind each step. This allows the agent to focus on specific aspects of the task while also ensuring that it completes all necessary steps.')])}"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:48:39.746509Z",
     "start_time": "2024-07-04T10:48:39.735906Z"
    }
   },
   "id": "5478958840ac4d71",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 18:48:48,893:INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-07-04 18:48:52,750:INFO - HTTP Request: POST https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": "'There are several common ways to perform task decomposition:\\n\\n  * Using LLMs with simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ.\"\\n  * Using task-specific instructions such as \"Write a story outline\" for writing a novel.\\n  * Using human inputs to guide the decomposition process.\\n\\nThese approaches help to make the task more manageable and allow the agent to focus on specific aspects of the task while still completing all necessary steps.'"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What are common ways of doing it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")[\"answer\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:48:53.390614Z",
     "start_time": "2024-07-04T10:48:48.438665Z"
    }
   },
   "id": "e171c0eebda44722",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'abc123': InMemoryChatMessageHistory(messages=[HumanMessage(content='What is Task Decomposition?'), AIMessage(content='Task Decomposition is a method used to break down complex tasks into smaller, more manageable ones. It involves breaking down a task into multiple steps or subtasks and then analyzing the logic behind each step. This allows the agent to focus on specific aspects of the task while also ensuring that it completes all necessary steps.'), HumanMessage(content='What are common ways of doing it?'), AIMessage(content='There are several common ways to perform task decomposition:\\n\\n  * Using LLMs with simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ.\"\\n  * Using task-specific instructions such as \"Write a story outline\" for writing a novel.\\n  * Using human inputs to guide the decomposition process.\\n\\nThese approaches help to make the task more manageable and allow the agent to focus on specific aspects of the task while still completing all necessary steps.')])}"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T10:48:58.380478Z",
     "start_time": "2024-07-04T10:48:58.372114Z"
    }
   },
   "id": "d69b4423e4dd09fe",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 文档地址\n",
    "# https://python.langchain.com/v0.1/docs/use_cases/question_answering/chat_history/"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7184fc2206aa04a3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
