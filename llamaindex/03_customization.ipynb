{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LlamaIndex 对 RAG 过程提供了全面的配置支持，允许开发者对整个过程进行个性化设置。常见的配置场景包括：\n",
    "# \n",
    "# 自定义文档分块\n",
    "# 自定义向量存储\n",
    "# 自定义检索\n",
    "# 指定 LLM\n",
    "# 指定响应模式\n",
    "# 指定流式响应\n",
    "# 注，个性化配置主要通过 LlamaIndex 提供的 ServiceContext 类实现。"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 自定义文档分块\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.service_context = ServiceContext.from_defaults(chunk_size=500)\n",
    "# service_context = ServiceContext.from_defaults(chunk_size=500)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1876a78abc4e9369"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Indexing & Embedding  索引和嵌入\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import Document\n",
    "\n",
    "# Indexing \n",
    "doc = Document(text=\"text\")\n",
    "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=10)\n",
    "\n",
    "Settings.text_splitter = text_splitter\n",
    "\n",
    "# per-index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    doc, transformations=[text_splitter]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3f888a9769ebf88"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    " # Storing# 储存\n",
    "# 一旦你有数据加载和索引，你可能会想存储它，以避免重新索引它的时间和成本。默认情况下，索引数据只存储在内存中。\n",
    "\n",
    "# Persisting to disk# 持久化到磁盘#\n",
    "index.storage_context.persist(persist_dir=\"<persist_dir>\")\n",
    "\n",
    "# graph.root_index.storage_context.persist(persist_dir=\"<persist_dir>\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "463de5c48d361e67"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "# 通过这样加载持久化索引来避免重新加载和重新索引数据：\n",
    "\n",
    "\n",
    "# rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"<persist_dir>\")\n",
    "\n",
    "# load index\n",
    "index = load_index_from_storage(storage_context)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c1f874351fd8ad4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Using Vector Stores# 使用矢量存储#\n",
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.legacy.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# load some documents\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "\n",
    "# initialize client, setting path to save data\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# create collection\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "\n",
    "# assign chroma as the vector_store to the context\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# create your index\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")\n",
    "\n",
    "# create a query engine and query\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is the meaning of life?\")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "457a63d7b93b3a52"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 如果你已经创建并存储了嵌入，你需要直接加载它们，而不需要加载你的文档或创建一个新的VectorStoreIndex：\n",
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.legacy.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "# initialize client\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# get collection\n",
    "chroma_collection = db.get_or_create_collection(\"quickstart\")\n",
    "\n",
    "# assign chroma as the vector_store to the context\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "# load your index from stored vectors\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store, storage_context=storage_context\n",
    ")\n",
    "\n",
    "# create a query engine\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What is llama2?\")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "494c14f8623de69c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 如果您已经创建了索引，则可以使用 insert 方法将新文档添加到索引中。\n",
    "# 插入文档或节点#\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex([])\n",
    "for doc in documents:\n",
    "    index.insert(doc)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2307c7e2d31c33bf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 现在，您已经加载了数据，构建了索引，并存储了该索引以供以后使用，您已经准备好进入LLM应用程序最重要的部分：查询。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "439ea77997505af6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\n",
    "    \"Write an email to the user given their background information.\"\n",
    ")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be433ef467470e17"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Customizing the stages of querying#\n",
    "# 自定义查询阶段#"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f4a28e9553f55b5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "# build index\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# configure retriever配置检索器\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# assemble query engine\n",
    "# Configuring node postprocessors#\n",
    "# 配置节点后处理器#\n",
    "# 支持先进的 Node 过滤和增强，可以进一步提高检索到的 Node 对象的相关性。这可以帮助减少LLM呼叫的时间/数量/成本或提高响应质量。\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    # SimilarityPostprocessor 通过设置相似性分数的阈值来过滤节点（因此仅支持基于嵌入的检索器）\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    ")\n",
    "\n",
    "# query\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73b0f9a01f768246"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 自定义检索中，我们可以通过参数指定查询引擎(Query Engine)在检索时请求的相似文档数。\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine(similarity_top_k=5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf6d7cbeb14ab3f9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 指定 LLM\n",
    "service_context = ServiceContext.from_defaults(llm=OpenAI())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c16c7751b5877595"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 指定响应模式\n",
    "query_engine = index.as_query_engine(response_mode='tree_summarize')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74b3f386754b10de"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 指定流式响应\n",
    "query_engine = index.as_query_engine(streaming=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee5a61145b8d38fc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c3f89dcdf4e8d9c2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
