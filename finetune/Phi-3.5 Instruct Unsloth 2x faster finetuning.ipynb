{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 您将学习如何进行数据准备，如何训练，如何运行模型，以及如何保存它（例如Llama.cpp）。\n",
    "# 参考文档：https://docs.unsloth.ai/tutorials/how-to-finetune-llama-3-and-export-to-ollama"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers==0.0.27\" trl peft accelerate bitsandbytes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c794a14a66d6d57"
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们支持 16 位 LoRA 或 4 位 QLoRA。两者都快 2 倍。\n",
    "可以设置为任何内容，因为我们通过 kaiokendev 的方法进行自动 RoPE 缩放。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "308ec5f87b589115"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",  # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",  # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\",  # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",  # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",  # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",  # Gemma 2x faster!\n",
    "]  # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Phi-3.5-mini-instruct\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3db18423e58aeeb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们现在添加 LoRA 适配器，因此我们只需要更新 1 到 10% 的所有参数！"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c5693ac86660eff"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\", ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",  # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  # We support rank stabilized LoRA\n",
    "    loftq_config=None,  # And LoftQ\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "673ad4976e4f1417"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Prep 数据准备"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "461f77bf283b83ec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们现在使用 Phi-3 格式进行对话风格微调。我们使用 ShareGPT 风格的 Open Assistant 对话。Phi-3 呈现多轮对话，如下所示：\n",
    "<|user|>\n",
    "Hi!<|end|>\n",
    "<|assistant|>\n",
    "Hello! How are you?<|end|>\n",
    "<|user|>\n",
    "I'm doing great! And you?<|end|>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a58edeebb4577f40"
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们使用我们的get_chat_template函数来获取正确的聊天模板。\n",
    "We support zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old and our own optimized unsloth template.\n",
    "\n",
    "注意：ShareGPT 使用 {“from”： “human”， “value” ： “Hi”} 而不是 {“role”： “user”， “content” ： “Hi”}，\n",
    "所以我们使用 mapping 来映射它。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91f91c075f483aa"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"phi-3\",  # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},  # ShareGPT style\n",
    ")\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
    "    return {\"text\": texts, }\n",
    "\n",
    "\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"philschmid/guanaco-sharegpt-style\", split=\"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True, )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f246a6288c530b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "让我们通过打印第 5 个元素来了解 Phi-3 格式的工作原理"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbecd11c8fde39eb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset[5][\"conversations\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58abab30d667e84d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(dataset[5][\"text\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68839536ec44ba83"
  },
  {
   "cell_type": "markdown",
   "source": [
    "如果您想制作自己的聊天模板，这也是可能的！您必须使用 Jinja 模板制度。我们提供了我们自己的精简版的 Unsloth 模板，\n",
    "我们发现该模板效率更高，并利用了 ChatML、Zephyr 和 Alpaca 样式。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba2484b4b1bda2f5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "unsloth_template = \\\n",
    "    \"{{ bos_token }}\" \\\n",
    "    \"{{ 'You are a helpful assistant to the user\\n' }}\" \\\n",
    "    \"{% for message in messages %}\" \\\n",
    "    \"{% if message['role'] == 'user' %}\" \\\n",
    "    \"{{ '>>> User: ' + message['content'] + '\\n' }}\" \\\n",
    "    \"{% elif message['role'] == 'assistant' %}\" \\\n",
    "    \"{{ '>>> Assistant: ' + message['content'] + eos_token + '\\n' }}\" \\\n",
    "    \"{% endif %}\" \\\n",
    "    \"{% endfor %}\" \\\n",
    "    \"{% if add_generation_prompt %}\" \\\n",
    "    \"{{ '>>> Assistant: ' }}\" \\\n",
    "    \"{% endif %}\"\n",
    "unsloth_eos_token = \"eos_token\"\n",
    "\n",
    "if False:\n",
    "    tokenizer = get_chat_template(\n",
    "        tokenizer,\n",
    "        chat_template=(unsloth_template, unsloth_eos_token,),  # You must provide a template and EOS token\n",
    "        mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},  # ShareGPT style\n",
    "        map_eos_token=True,  # Maps <|im_end|> to </s> instead\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "254dd84c9a57b891"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the model 训练模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d680dc7b106e97f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "现在让我们使用 Huggingface TRL 的 SFTTrainer！更多文档请见：TRL SFT 文档。我们执行 60 个步骤来加快速度，\n",
    "但您可以设置 num_train_epochs=1 进行完整运行，并关闭 max_steps=None。我们还支持 TRL 的 DPOTrainer！"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "351328ae999e810b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,  # Can make training 5x faster for short sequences.\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        max_steps=60,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "    ),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd4c33c65c4d54f7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show current memory stats"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5424460f9af928f2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "173352bfb76d8ef5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5058f421b946d285"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Show final memory and time stats"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0b5f87d57ac4aef"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime'] / 60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efb341be691bf4d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inference 推理"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c14bb45806739d0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "让我们运行模型！由于我们使用的是 Phi-3，因此请使用 apply_chat_template 并将 add_generation_prompt设置为 True 进行推理。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8711a39c498c77da"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"phi-3\",  # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},  # ShareGPT style\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,  # Must add for generation\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids=inputs, max_new_tokens=64, use_cache=True)\n",
    "tokenizer.batch_decode(outputs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c188b848bb94eceb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "您还可以使用 TextStreamer 进行连续推理 - 这样您就可以逐个令牌查看生成，而不是一直等待！"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b590bbba249e0ff"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,  # Must add for generation\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "_ = model.generate(input_ids=inputs, streamer=text_streamer, max_new_tokens=128, use_cache=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad2d0b7b33bf0968"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 保存、加载微调模型\n",
    "Saving, loading finetuned models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8359c1901d9dcc7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "要将最终模型保存为 LoRA 适配器，请使用 Huggingface 的 push_to_hub 进行在线保存，或使用 save_pretrained 进行本地保存。\n",
    "\n",
    "这只会保存 LoRA 适配器， 而不是完整模型.要保存为 16 位或 GGUF，请向下滚动！"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27e52bdbe8ca5d36"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "model.push_to_hub(\"coderjuzi/lora_model\", token=\" \")  # Online saving\n",
    "tokenizer.push_to_hub(\"coderjuzi/lora_model\", token=\" \")  # Online saving"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f62542be41ebc50b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "现在，如果您想加载我们刚刚保存用于推理的 LoRA 适配器，请将 False 设置为 True："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3306671b87c7b23c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if False:\n",
    "    from unsloth import FastLanguageModel\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"lora_model\",  # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"What is a famous tall tower in Paris?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,  # Must add for generation\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "_ = model.generate(input_ids=inputs, streamer=text_streamer, max_new_tokens=128, use_cache=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8197479c1a7c4c5a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "您还可以使用 Hugging Face 的 AutoModelForPeftCausalLM。仅当您未安装 unsloth 时才使用此功能。\n",
    "它可能会慢得无可救药，因为不支持 4 位模型下载，而 Unsloth 的推理速度快 2 倍。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93763b387f80e6b4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if False:\n",
    "    # I highly do NOT suggest - use Unsloth if possible\n",
    "    from peft import AutoPeftModelForCausalLM\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "        \"lora_model\",  # YOUR MODEL YOU USED FOR TRAINING\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8eba2e8f3d061575"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 为 VLLM 保存到 float16\n",
    "Saving to float16 for VLLM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "449aa30cab10d8b9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们也支持直接保存到 float16。为 float16 选择 merged_16bit，为 int4 选择 merged_4bit。我们还允许使用 Lora 适配器作为后备.使用push_to_hub_merged上传到您的Hugging Face帐户！您可以去 https://huggingface.co/settings/tokens 获取您的个人代币。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5e4a7e862a73dd9a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\", )\n",
    "if True: model.push_to_hub_merged(\"coderjuzi/Phi-3.5-16bit-vllm\", tokenizer, save_method=\"merged_16bit\",\n",
    "                                  token=\" \")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_4bit\", )\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"merged_4bit\",\n",
    "                                   token=\" \")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method=\"lora\", )\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method=\"lora\",\n",
    "                                   token=\" \")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0042050b6559662"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GGUF / llama.cpp 转换\n",
    "\n",
    "要保存到 GGUF / llama.cpp，我们现在原生支持它！我们克隆llama.cpp，并默认将其保存为 q8_0。\n",
    "我们允许所有方法，例如q4_k_m。使用 save_pretrained_gguf 进行本地保存，使用 push_to_hub_gguf 上传到 HF。\n",
    "\n",
    "一些受支持的量化方法（完整列表在我们的 Wiki 页面上）：\n",
    "\n",
    "q8_0 - Fast conversion. High resource use, but generally acceptable.\n",
    "- 快速转换。资源使用率高，但总体上可以接受。\n",
    "q4_k_m - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "-推荐。使用 Q6_K 表示 attention.wv 和 feed_forward.w2 张量的一半，否则Q4_K。\n",
    "q5_k_m - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
    "-推荐。使用 Q6_K 表示 attention.wv 和 feed_forward.w2 张量的一半，否则Q5_K。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfc4f6430c314d5f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, )\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token=\"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method=\"f16\", token=\"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method=\"q4_k_m\", token=\"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\",  # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method=[\"q4_k_m\", \"q8_0\", \"q5_k_m\", ],\n",
    "        token=\"\",  # Get a token at https://huggingface.co/settings/tokens\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b514c1a4a6145ed0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "现在，在 llama.cpp 或基于 UI 的系统（如 GPT4All）中使用 model-unsloth.gguf 文件或 model-unsloth-Q4_K_M.gguf 文件。你可以通过去安装 GPT4All 这里."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe414b7a4b46f4e7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
